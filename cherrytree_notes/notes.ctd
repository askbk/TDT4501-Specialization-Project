<?xml version="1.0" encoding="UTF-8"?>
<cherrytree>
  <bookmarks list=""/>
  <node name="Specialization project" unique_id="1" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="1" foreground="" ts_creation="1630387901" ts_lastsave="1633168889">
    <rich_text scale="h1">DevOps for AI</rich_text>
    <rich_text>



</rich_text>
    <rich_text scale="h2">Research question drafts by category:</rich_text>
    <rich_text>
</rich_text>
    <rich_text scale="h3">Architecture</rich_text>
    <rich_text>
- message-based training architecture
</rich_text>
    <rich_text scale="h3">Versioning</rich_text>
    <rich_text>
- How to version online models?
   → Typical architecture for contextual bandit: </rich_text>
    <rich_text link="webs https://youtu.be/um6Sq5EhW6A?t=3117">https://youtu.be/um6Sq5EhW6A?t=3117</rich_text>
    <rich_text>
   → What is the solution for other types of machine learning systems?
- How to version projects? (data + model + code)
   → Git + DVC
   → Pachyderm
- have we deployed the correct model?

</rich_text>
    <rich_text scale="h3">Dependencies</rich_text>
    <rich_text>
- How to find the transitive closure of all data dependencies? (HTDMLS)
- How to deploy model with breaking changes?
   → E.g.: Schema for input is changed
- How to detect changes in data dependencies?
   → Detect changes in data value/schema that production model relies on from external systems
   → Detect changes in system that depend on model
- How to measure the change to the system when a new change is deployed? (HTDMLS)
- How to predict whether improvement to a model will degrade another model in the system? (HTDMLS)

</rich_text>
    <rich_text scale="h3">Monitorability</rich_text>
    <rich_text>
- How to architect for monitorability?
   → see how it is done in devops

</rich_text>
    <rich_text scale="h3">Training, updating and retraining</rich_text>
    <rich_text>
- Model abstraction to decouple training and operating environments
- Model drift: when should we retrain?
- hyperparameter tuning

</rich_text>
    <rich_text scale="h3">Misc</rich_text>
    <rich_text>
- How to handle complex data ownership?
- How to enable language-agnostic MLOps?
- Broad topic: MLOps for online learning systems
   → Governance
   → Model-free reinforcement learning
   → Model versioning
- DevSecOps - how to ensure security and privacy in MLops?
- how to handle adversarial perturbations?
   → how to detect? what to do about it?
- Managing competitive training of model variants against each other in dev environments
   → for online learning models
- Emergency cut out
- how to solve grade-your-own-exam anti-pattern in MLOps context?
- how to communicate model uncertainty?

- how to do deployments in a federated learning context?
- how to build CI/CD pipelines for federated learning?
- monitorability, auditability, etc in federated learning?
- research questions
   → stakeholders
   → impact

- tools for full lifecycle:
   → polyaxon
   → valohai</rich_text>
    <node name="SEMLA webinar notes" unique_id="4" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630390311" ts_lastsave="1631185334">
      <rich_text>- Versioning
   → What model version did i run? Which data was it trained on?
   → Track metadata across systems
- Monitoring model drift
- Alerting
- Logging
- Auditing
- Model versioning
- Heterogeneous systems
- Repeatability, reproducibility
- Abstract infrastructure away
- Continuous integration, deployment
- Composability
- “Hidden technical debt in machine learning systems” - google
- Algorithmia white papers
- “Foundations for ML at Scale” - Peter Skomoroch
- Performance SLAs
- Accounting
- Daily retraining and redeployment with new data
- Infrastructure tasks during training and inference
- Toughtworks “Get MLops right”
- Data changes over time:
   → Schema
   → Sampling over time
   → Volume
- Model changes over time:
   → Algorithms
   → More training
   → Experiments
- Code changes over time:
   → Business need
   → Bug fixes
   → Configuration
- Multiple repos for data, schema, model, container
   → Meta-repo for tracking dependencies between repos?
- Model drift: data drift vs concept drift
- </rich_text>
      <rich_text underline="single" link="webs https://martinfowler.com/articles/cd4ml.html" foreground="#1155cc">https://martinfowler.com/articles/cd4ml.html</rich_text>
      <rich_text>
- Challenges and opportunities for architecting ML systems
   → Architecture patterns and tactics for ML-important quality attributes
   → Monitorability as a driving quality attribute
   → Co-architecting and co-versioning
- Quality attribute metrics for fairness/ethics</rich_text>
    </node>
    <node name="Papers" unique_id="5" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630485213" ts_lastsave="1634115427">
      <node name="Papers to read" unique_id="3" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630389156" ts_lastsave="1631280378">
        <rich_text>AI Engineering - Software Engineering for AI (WAIN):
- </rich_text>
        <rich_text link="webs https://ieeexplore.ieee.org/xpl/conhome/9473903/proceeding">https://ieeexplore.ieee.org/xpl/conhome/9473903/proceeding</rich_text>
        <rich_text>

Overview:
</rich_text>
        <rich_text strikethrough="true">- WHy do 80% blabal: </rich_text>
        <rich_text strikethrough="true" link="webs https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/">https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/</rich_text>
        <rich_text>
</rich_text>
        <rich_text strikethrough="true">- Using AntiPatterns to avoid MLOps Mistakes: </rich_text>
        <rich_text strikethrough="true" link="webs https://arxiv.org/pdf/2107.00079.pdf">https://arxiv.org/pdf/2107.00079.pdf</rich_text>
        <rich_text>
</rich_text>
        <rich_text strikethrough="true">- Who Needs MLOps: What Data Scientists Seek to Accomplish and How Can MLOps Help?: </rich_text>
        <rich_text strikethrough="true" link="webs https://arxiv.org/pdf/2103.08942.pdf">https://arxiv.org/pdf/2103.08942.pdf</rich_text>
        <rich_text>
- </rich_text>
        <rich_text strikethrough="true">MLOps Challenges in Multi-Organization Setup: Experiences from Two Real-World Cases: </rich_text>
        <rich_text strikethrough="true" link="webs https://arxiv.org/pdf/2103.08937.pdf">https://arxiv.org/pdf/2103.08937.pdf</rich_text>
        <rich_text>
- A Data Quality-Driven View of MLOps: </rich_text>
        <rich_text link="webs https://arxiv.org/pdf/2102.07750.pdf">https://arxiv.org/pdf/2102.07750.pdf</rich_text>
        <rich_text>
- The Collective Knowledge project: making ML models more portable and reproducible with open APIs, reusable best practices and MLOps: </rich_text>
        <rich_text link="webs https://arxiv.org/pdf/2006.07161.pdf">https://arxiv.org/pdf/2006.07161.pdf</rich_text>
        <rich_text>
- Software Engineering for Machine Learning: A Case Study: </rich_text>
        <rich_text link="webs https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/">https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/</rich_text>
        <rich_text>
- Challenges in the deployment and operation of machine learning in practice: </rich_text>
        <rich_text link="webs https://www.researchgate.net/profile/Lucas-Baier/publication/332996647_CHALLENGES_IN_THE_DEPLOYMENT_AND_OPERATION_OF_MACHINE_LEARNING_IN_PRACTICE/links/5cd57a7c92851c4eab924c03/CHALLENGES-IN-THE-DEPLOYMENT-AND-OPERATION-OF-MACHINE-LEARNING-IN-PRACTICE.pdf">https://www.researchgate.net/profile/Lucas-Baier/publication/332996647_CHALLENGES_IN_THE_DEPLOYMENT_AND_OPERATION_OF_MACHINE_LEARNING_IN_PRACTICE/links/5cd57a7c92851c4eab924c03/CHALLENGES-IN-THE-DEPLOYMENT-AND-OPERATION-OF-MACHINE-LEARNING-IN-PRACTICE.pdf</rich_text>
        <rich_text>
</rich_text>
        <rich_text strikethrough="true">- Challenges in Deploying Machine Learning: a Survey of Case Studies: </rich_text>
        <rich_text strikethrough="true" link="webs https://arxiv.org/pdf/2011.09926">https://arxiv.org/pdf/2011.09926</rich_text>
        <rich_text>
Monitoring:
- Monitoring machine learning models in production: </rich_text>
        <rich_text link="webs https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/">https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/</rich_text>
        <rich_text>
- Monitoring machine learning models: </rich_text>
        <rich_text link="webs https://towardsdatascience.com/monitoring-machine-learning-models-62d5833c7ecc">https://towardsdatascience.com/monitoring-machine-learning-models-62d5833c7ecc</rich_text>
        <rich_text>
- </rich_text>
        <rich_text link="webs https://ai.google/research/pubs/pub46555">The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction</rich_text>
        <rich_text> (Google)
- Monitoring Azure Machine Learning: </rich_text>
        <rich_text link="webs https://docs.microsoft.com/en-us/azure/machine-learning/monitor-azure-machine-learning">https://docs.microsoft.com/en-us/azure/machine-learning/monitor-azure-machine-learning</rich_text>
        <rich_text>
- Deploying Machine Learning Models for Public Policy: A Framework: </rich_text>
        <rich_text link="webs https://dl.acm.org/doi/pdf/10.1145/3219819.3219911">https://dl.acm.org/doi/pdf/10.1145/3219819.3219911</rich_text>
        <rich_text>

- books:
   → accelerate: the science of lean software
   → the phoenix framework
   → deploying AI in the enterprise: </rich_text>
        <rich_text link="webs https://link.springer.com/book/10.1007%2F978-1-4842-6206-1">https://link.springer.com/book/10.1007%2F978-1-4842-6206-1</rich_text>
        <rich_text>

A lot of papers: </rich_text>
        <rich_text link="webs https://github.com/SE-ML/awesome-seml">https://github.com/SE-ML/awesome-seml</rich_text>
        <rich_text>

On conducting MLRs (28 pages): </rich_text>
        <rich_text link="webs https://arxiv.org/ftp/arxiv/papers/1707/1707.02553.pdf">Guidelines for conducting MLRs-Sept 15-2.docx (arxiv.org)</rich_text>
      </node>
      <node name="Seed papers" unique_id="25" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634115427" ts_lastsave="1635250497">
        <rich_text justification="left"></rich_text>
        <table char_offset="0" justification="left" col_min="97" col_max="97" col_widths="202,0,67,172,1702,52">
          <row>
            <cell>Software engineering challenges of deep learning</cell>
            <cell>Arpteg et al.</cell>
            <cell>2018</cell>
            <cell>https://doi.org/10.1109/SEAA.2018.00018</cell>
            <cell>- challenge in dependency management as a result of rapidly improving hardware (new GPUs released frequently). May cause issues with reproducibility. Incurs engineering costs with keeping HW and SW up-to-date.
- monitoring and logging to detect data drift. Choosing what metrics to monitor can be challenging.
- keeping glue code up to date. Keeping up to date with external changes in cloud services.
- privacy and data safety. “more work is needed to preserve the privacy and safety of sensitive datasets while still being able to efficiently perform data exploration, develop models, and troubleshoot problems”.
   personal data needs to be anonymized and encrypted. This includes any information that might be used to reverse-engineer the identify of the user, e.g. gender, age, and city.

Even though this information would be useful to have and could improve the performance of the model, it is encrypted and can be difficult to make use of when training models and building the system.</cell>
            <cell>1</cell>
          </row>
          <row>
            <cell>Applying deep learning to AirBnB search</cell>
            <cell>Haldar et al.</cell>
            <cell>2019</cell>
            <cell>https://doi.org/10.1145/3292500.3330658</cell>
            <cell>- “Towards the beginning of 2017 when we started shipping the TensorFlow models to production, we found no efficient solution to score the models within a Java stack. Typically a back and forth conversion of the data between Java and another language was required and the latency introduced in the process was a blocker for us.”
- Interoperation of Java, Scala, Tensorflow, Spark, Thrift, AWS</cell>
            <cell>2</cell>
          </row>
          <row>
            <cell>Serving deep learning models in a serverless platform</cell>
            <cell>Ishakian et al.</cell>
            <cell>2018</cell>
            <cell>https://doi.org/10.1109/IC2E.2018.00052</cell>
            <cell>- warm serverless functions give acceptable latency, while latency in cold-start scenarios are significant
- serverless platforms need to support more stateful workloads to improve model serving performance (minimum number to keep warm, duration to keep warm, GPU access, etc)</cell>
            <cell>3</cell>
          </row>
          <row>
            <cell>Pest Management In Cotton Farms: An AI-System Case Study from the Global South</cell>
            <cell>Dalmia et al.</cell>
            <cell>2020</cell>
            <cell>https://doi.org/10.1145/3394486.3403363</cell>
            <cell>- model was deployed to the cloud, but connectivity issues for rural users required that the model be compressed and ported to a mobile application. This resulted in a smaller and more energy-efficient model, at the cost of increased mean average error.</cell>
            <cell>4</cell>
          </row>
          <row>
            <cell>A Berkeley View of Systems Challenges for AI</cell>
            <cell>Stoica et al.</cell>
            <cell>2017</cell>
            <cell>https://arxiv.org/abs/1712.05855</cell>
            <cell>- heterogeneity in edge devices makes application development much more challenging for edge-deployed models.
- HW/SW updates in edge devices are much more infrequent than in cloud environments
- proposed solutions: 1) port to all the different device platforms/"compilers and JIT technologies to effciently compile on-the-fy complex algorithms and run them on
edge devices.", 2) "design AI systems that are well-suited to partitioned execution across the cloud and the edge."
- proposed research area: "Design cloud-edge AI systems that (1) leverage the edge to reduce latency, improve safety and security, and implement intelligent data retention techniques, and (2) leverage the cloud to share data and models across edge devices, train sophisticated computation-intensive models, and take high quality decisions."</cell>
            <cell>5</cell>
          </row>
          <row>
            <cell>Engineering AI Systems: A Research Agenda</cell>
            <cell>Bosch et al.</cell>
            <cell>2021</cell>
            <cell>10.4018/978-1-7998-5101-1.ch001</cell>
            <cell>- deployment is highly underestimated area. companies struggle with: monitoring, logging, testing, troubleshooting, resource limitations, significant amounts of glue code
Primary research challenges identified:
- federated learning infrastructure: solutions are needed for federated learning and the sharing of model parameters such as neural network weights as well as selected data sets that, for instance, represent cases not well handled by the central model. Federated learning requires an infrastructure to achieve the required quality attributes and to efficiently and securely share models and data.
- storage and computing infrastructure: many companies use internal infrastructure instead of cloud services. can be because of legal constraints, costs or quality attributes.
- deployment infrastructure: it is important for a deployment infrastructure to reliably deploy subsequent versions of models, measure their performance, raise warnings and initiate rollbacks in the case of anomalous behaviour.</cell>
            <cell>6</cell>
          </row>
          <row>
            <cell>ModelOps: Cloud-based lifecycle management for reliable and trusted AI</cell>
            <cell>Hummer et al.</cell>
            <cell>2019</cell>
            <cell>https://doi.org/10.1109/IC2E.2019.00025</cell>
            <cell>“systematic lifecycle support-including continuous development, training, testing, and deployment of models-and continuous integration [2] for AI is still in its infancy. We think one reason for this is the lack of tools and methodologies to support the development lifecycles of AI solutions, spanning data preparation, model design and training, application development, quality checks (e.g., security, bias, compliance, etc.), deployment, monitoring, and feedback, as well as the reproducibility and auditability of the entire process.”
- pluggability: “Lifecycle services must be easily pluggable and customizable, e.g. bias or robustness checks. some AI pipelines still require human in the loop.”
- reusability: keep config effort to a minimum. users should be able to use pre-configured templates and patterns for pipelines and lifecycle capabilities.
- flexibility: The system needs to meet developers and data scientists where they are, and integrate with the tools and services they already use and are comfortable with.
- scalability: The system needs to provide scalability across different dimensions - including model metadata versioning, parallel pipeline executions, event processing, and model performance monitoring.
- hybrid environments: While there is a general move to the cloud, a significant number of AI systems use on-premise servers, dedicated clusters, edge devices, or a combination of these. Resource and security heterogeneity in such hybrid environments introduce a number of challenges.
- fault tolerance: As ModelOps pipelines plug together a wide range of tools and infrastructure, many things can fail. For example, we found that 8% of all data preprocessing tasks in WML fail or finish with errors, and therefore would stall the subsequent training of a model. This is particularly problematic for automated pipelines of critical models.</cell>
            <cell>7</cell>
          </row>
          <row>
            <cell>How do engineers perceive blabla</cell>
            <cell>Ishikawa and Yoshioka</cell>
            <cell>2019</cell>
            <cell>https://doi.org/10.1109/CESSER-IP.2019.00009</cell>
            <cell>“concerns for practitioners include difficulties with monitoring unexpected behavior, fault localization, and maintainability for continuous engineering. Such problems are only expected to grow, as many products and services will be coming into use and the demand for quality will increase.”</cell>
            <cell>8</cell>
          </row>
          <row>
            <cell>Don't Forget Your Roots! Using Provenance Data for Transparent and Explainable Development of Machine Learning Models</cell>
            <cell>Jentzsch and Hochgeschwender</cell>
            <cell>2019</cell>
            <cell>https://doi.org/10.1109/ASEW.2019.00025</cell>
            <cell>"Instead of model transparency, we argue process transparency of the development and model training to be the key to XAI. The whole ML development process, from data acquisition to prediction, consists of numerous mutual dependant decisions that each may determine the final outcome sensitively."
- i.e.: "data collection, data preparation, model selection, model training, model evaluation or hyperparameter tuning"
→ me: deployment infra must carry provenance information to support explainability</cell>
            <cell>9</cell>
          </row>
          <row>
            <cell>Toward a Framework for Highly Automated Vehicle Safety Validation</cell>
            <cell>Koopman and Wagner</cell>
            <cell>2018</cell>
            <cell>https://doi.org/10.4271/2018-01-1071</cell>
            <cell>proposes: 
- “A run-time monitoring approach to managing identified risks, catching assumption violations and unknown unknowns as they arise in fielded systems.”
- Observability test points. Should be able to verify that system acts for the right reason.</cell>
            <cell>10</cell>
          </row>
          <row>
            <cell>Data Scientists in Software Teams: State of the Art and Challenges</cell>
            <cell>Kim et al.</cell>
            <cell>2018</cell>
            <cell>https://doi.org/10.1109/TSE.2017.2754374</cell>
            <cell>the large number of tools is a major challenge.
“respondents also expressed frustration about having too many tools and incompatible tools”
→ simply creating another tool should not be done lightly. the large number of existing tools is already a point of frustration for practitioners.</cell>
            <cell>11</cell>
          </row>
          <row>
            <cell>Trustworthy AI in the Age of Pervasive Computing and Big Data</cell>
            <cell>Kumar et al.</cell>
            <cell>2020</cell>
            <cell>https://doi.org/10.1109/PerComWorkshops48775.2020.9156127</cell>
            <cell>Proposes a framework for trustworthy AI systems in the age of pervasive data collection and computing.

“Failure Identification &amp; Reliability Monitoring are important to ensure reliability in AI systems"
"To prevent failures in AI systems, the current state-of-the-art methods try to proactively identify likely sources of error resulting from 1) bad or inadequate data [17], 2) differences or shifts in environment [18], 3) model associated errors [11], or 4) poor reporting [19] and develop methods that correct for these in advance. After deployment, reliability mechanisms assess the model output for each new input and reject the unreliable output based on the auditing criteria of the density principle and the local fit principle [20]. Model maintenance requires detecting when updates to the model are necessary"
→ </cell>
            <cell>12</cell>
          </row>
          <row>
            <cell>Related Pins at Pinterest: The Evolution of a Real-World Recommender System</cell>
            <cell>Liu et al.</cell>
            <cell>2017</cell>
            <cell>https://doi.org/10.1145/3041021.3054202</cell>
            <cell>multiple different models/components that depend on eachother, meaning that changing a model requires that the whole system is retrained. 
"We identify unique interdependencies that made it hard to reason about changes in Related Pins, and propose to mitigate these issues by automated joint training of system components."

Improvements to raw data can harm results: “Currently we must manually retrain our model with updated raw data, and deploy the new model and raw data into production at the same time. In addition to being time consuming, this solution is less-than-ideal since it requires us to know of the upstream change. Ideally, we would automate the continual retraining of our model, which would incorporate any change in upstream data.”</cell>
            <cell>13</cell>
          </row>
          <row>
            <cell>Developing and Operating Artificial Intelligence Models in Trustworthy Autonomous Systems</cell>
            <cell>Martínez-Fernández et al.</cell>
            <cell>2021</cell>
            <cell>https://doi.org/10.1007/978-3-030-75018-3_14</cell>
            <cell>“However, solutions or frameworks supporting the management of multiple instances of AI models deployed in different context-aware environments are scarce. Context is the missing piece in the AI lifecycle [15].”
“AI models are usually deployed as part of embedded systems, which require additional techniques for continuous deployment, such as Over-the-Air updates [18]. However, as aforementioned, none of the these approaches have a focus on trustworthiness nor context-specific AI model deployment.”
Proposed research direction: “Providing Intelligent and Context-Aware Techniques to Deploy Updated AI Models in AS (Autonomous Systems) Instances”

“Bringing Together the Development and Operation of AI Models in Trustworthy AS into a Holistic Lifecycle with Tool Support.”
“This direction includes the development of AI-specific, independent, and loosely coupled software components (ready to be integrated into companies’ development and operational environments) for the three directions we presented above.”

Proposes a holistic DevOps approach for AI in AS.</cell>
            <cell>14</cell>
          </row>
          <row>
            <cell>Machine Learning Software Engineering in Practice: An Industrial Case Study</cell>
            <cell>Rahman et al.</cell>
            <cell>2019</cell>
            <cell>https://arxiv.org/abs/1906.07154</cell>
            <cell>"Post-deployment monitoring of models is important. Based on the performance, models may need to be retrained which may initiate a maintenance cycle."
“Integration of ML components must ensure the functional integrity of the ML applications.”
“The deployment should consider the compatibility and differences in development and production platforms.”
"The deployment must ensure a smooth roll-out of the existing system without affecting the user or the business
process."</cell>
            <cell>15</cell>
          </row>
          <row>
            <cell>On challenges in machine learning model management</cell>
            <cell>Schelter et al.</cell>
            <cell>2019</cell>
            <cell>http://web.kaust.edu.sa/Faculty/MarcoCanini/classes/CS290E/F19/papers/challenges.pdf</cell>
            <cell>"The ability to back-test the accuracy of models over time is crucial in many real-world usages
of ML. Models evolve continuously as data changes, methods improve or software dependencies change. Every
time such a change occurs, model performance must be re-validated."
"Apart from validation of models in offline backtests, assessing the performance of models in production is
crucial."
"Multi-language code bases are hard to deploy later on, as they require setups with many different components that need to be orchestrated. E.g., a Spark cluster must be spun up for pre-processing, afterwards the data must be moved to a single machine for model training, and the cluster must be torn down afterwards. An orthogonal problem is the efficient and reliable exchange of data between the components of the system written in different languages."</cell>
            <cell>16</cell>
          </row>
          <row>
            <cell>An Empirical Study of Software Architecture for Machine Learning</cell>
            <cell>Serban and Visser</cell>
            <cell>2021</cell>
            <cell>https://arxiv.org/abs/2105.12422</cell>
            <cell>SLR+Interviews+Survey
"Design for automatic continuous retraining. Use CI/CD. Use automatic rollback. Use infrastructure-as-code. Adopt standard release processes."
"Encapsulate ML components in identifiable modules/services. Use authentication and access control. Log consumers of ML components."
"Design for batch processing (training) and stream processing (serving), i.e., lambda architecture. Physically isolate the workloads. Use virtualisation."
"Design for traceability and reproducibility; log pointers to versioned artefacts, version configurations, models and data."
</cell>
            <cell>17</cell>
          </row>
          <row>
            <cell>An Empirical Study of Common Challenges in Developing Deep Learning Applications</cell>
            <cell>Zhang et al.</cell>
            <cell>2019</cell>
            <cell>https://doi.org/10.1109/ISSRE.2019.00020</cell>
            <cell>Looks at StackOverflow questions.
Category “deployment and migration” is quite common question type.
“Behavior inconsistency often occurs when migrating a model between different frameworks or deploying a model to a different platform (e.g., Android, IOS, web browsers). However, it is hard to uncover and diagnose such behavioral inconsistency due to a lack of tool support.”</cell>
            <cell>18</cell>
          </row>
          <row>
            <cell>Machine Learning System Architectural Pattern for Improving Operational Stability</cell>
            <cell>Yokoyama</cell>
            <cell>2019</cell>
            <cell>https://doi.org/10.1109/ICSA-C.2019.00055</cell>
            <cell>Discusses a bit of CI/CD stuff.
"Reproducibility of the inference engine is necessary to realize continuous deployment. Unlike a usual program, the inference engine depends on various data, algorithms, and hyperparameters. Therefore, if the depending element cannot be prepared accurately, a past inference engine might not be able to be reproduced. To address this problem, version control is necessary to manage the data, algorithms, and hyperparameters. DVC [16] and ModelHub [12] [13] support such tasks."</cell>
            <cell>20</cell>
          </row>
          <row>
            <cell>An Architecture for Agile Machine Learning in Real-Time Applications</cell>
            <cell>Schleier-Smith</cell>
            <cell>2015</cell>
            <cell>https://doi.org/10.1145/2783258.2788628</cell>
            <cell>"We also have a well-defined path to deploying new models to production: we start out by playing back history, rolling forward in time to the present, then transition seamlessly to real-time streaming. By construction, our model code works just the same with inputs that are months old as with those that are milliseconds old, making it practical to use a single model description in both development and production deployment."
"[We] invest heavily in the framework for data transformation, production state management, model description, model training, backtesting and validation, production monitoring, and production experimentation"

Challenge identified: "Long deployment cycles: Any algorithm changes required writing a large amount of software: SQL to extract historical data, Java code in models, Java code for processing real-time updates, often PHP code and more SQL to change how data was collected. For live experiments we also needed to consider how new and old implementations would coexist in production."</cell>
            <cell></cell>
          </row>
          <row>
            <cell>Title</cell>
            <cell>Authors</cell>
            <cell>Year</cell>
            <cell>Link</cell>
            <cell>Comment</cell>
            <cell></cell>
          </row>
        </table>
      </node>
    </node>
    <node name="Useful for thesis" unique_id="12" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1631184972" ts_lastsave="1633605763">
      <rich_text>Federated Learning: Privacy and incentive. (2020). </rich_text>
      <rich_text link="webs https://link.springer.com/book/10.1007%2F978-3-030-63076-8">https://link.springer.com/book/10.1007%2F978-3-030-63076-8</rich_text>
      <rich_text>
</rich_text>
      <rich_text link="webs https://federated.fastforwardlabs.com/">https://federated.fastforwardlabs.com/</rich_text>
      <rich_text>
</rich_text>
      <rich_text link="webs https://federated.withgoogle.com/">https://federated.withgoogle.com/</rich_text>
      <rich_text>
Elixir for programming distributed systems
Lwakatare et al 2020, mentions deployment of federated learning:
	“According to practitioners' experience [P42], federated learning requires specialized deployment frameworks capable to handle changes at the edge.” -

</rich_text>
      <rich_text link="webs https://www.openmined.org/">https://www.openmined.org/</rich_text>
      <rich_text>
</rich_text>
      <rich_text link="webs https://github.com/FederatedAI/FATE">https://github.com/FederatedAI/FATE</rich_text>
      <rich_text>
</rich_text>
      <rich_text link="webs https://github.com/scaleoutsystems/fedn">https://github.com/scaleoutsystems/fedn</rich_text>
    </node>
    <node name="Research plan" unique_id="16" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1631529882" ts_lastsave="1631697114">
      <rich_text>Purpose
- deployment of machine learning models is challenging from a technical perspective
- the field of MLops is very young (&lt;5 years) and is developing rapidly, with many new tools being launched every year
- this is a problem for all organizations that are deploying or wish to deploy machine learning models into production
- literature reviews in the field of MLOps is scarce
   → Only one systematic literature review of the entire MLops cycle (John et al)
   → a couple of reviews of case studies
   → microsoft paper
- problems with earlier research:
   → limited grey literature coverage
   → out of date
- I will conduct a systematic multivocal literature review to solve the above two problems
- main research question: what is the state of the art in deploying machine learning models?
   → subquestion 1: what are the main challenges and pain points in deploying machine learning models?
   → subquestion 2: ??
   →
Contributions
Research method
Participants
Research paradigm
Final deliverables and dissemination</rich_text>
    </node>
    <node name="Supervision meetings" unique_id="13" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1631197091" ts_lastsave="1635161521">
      <node name="26.08.2021" unique_id="14" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1631197176" ts_lastsave="1631197290">
        <rich_text>Who will i help? Find out stakeholders
- Data scientist? Software developers? managers?


Find pain point
- Who cares about e.g. deployment time
- Who will care? Why? Propose reasons
- Do literature review. Does anyone know why the problem exists? Find out why.
- Is the reason for the problem known? If not, find out reason first. If known, address the problem directly.
- Is it big problem or trivial?

</rich_text>
        <rich_text weight="heavy">For next meeting:</rich_text>
        <rich_text>
- 4 Research questions
- Detailed analysis
   → Stakeholders
   → Problem
   → Impact



</rich_text>
      </node>
      <node name="10.09.2021 - Discussing research questions" unique_id="15" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1631197290" ts_lastsave="1631280229">
        <rich_text>- Project: Systematic literature review of e.g. CI/CD in MLOps.
   → Look at what systematic literature reviews on CI/CD in MLOps already exist
   → Find out how to stand out so you make a contribution
      ⇒ Include different sets of papers, e.g. industrial if that hasn't been done before

Next time, create plan for systematic literature review.

Read guidelines for conducting multivocal literature reviews: </rich_text>
        <rich_text link="webs https://arxiv.org/ftp/arxiv/papers/1707/1707.02553.pdf">Microsoft Word - Guidelines for conducting MLRs-Sept 15-2.docx (arxiv.org)</rich_text>
        <rich_text>
(Read book in TDT39?)
</rich_text>
      </node>
      <node name="24.09.2021 - Input on research plan" unique_id="18" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1632479208" ts_lastsave="1632909308">
        <rich_text>- make table comparing expected output with output of existing MLR
- semantic analysis step in data synthesis
- oria for querying
   → searches IEEE, ACM, etc
   → filters duplicates
- for next time:
   → do some pilot queries
   → construct selection criteria
   → discuss with bill next time</rich_text>
      </node>
      <node name="5.10.2021" unique_id="22" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1633335688" ts_lastsave="1633961352">
        <rich_text>Lwakatare et al (2020):
- Creates a conceptual model of five stages of ML development process improvement

- Focus on tools for deployment?
   → difficult to perform literature review? academic papers rarely mention tools(?)

Modified research questions:
- What is the state of the art and state of the practice in deploying ML models?
   → SQ1: How are ML models being deployed in real-world applications?
   → SQ2: What tools are used to deploy ML models? focus on features
   → SQ3: What are the main challenges and pain points in deploying ML models?
   → What are the limitations of the tools?

- New study focus provides additional inclusion criteria for study selection:
   → Paper has to mention specific tools used


- Until next time:
   → Do pilot search, find 10 maybe 20 papers related to tools to make sure I am in good shape


- Papers found:
   → </rich_text>
        <rich_text link="webs https://doi.org/10.1109/ICSE-SEIP.2019.00042">https://doi.org/10.1109/ICSE-SEIP.2019.00042</rich_text>
        <rich_text>
   → </rich_text>
        <rich_text link="webs https://doi.org/10.1007/978-3-030-19034-7_14">https://doi.org/10.1007/978-3-030-19034-7_14</rich_text>
        <rich_text>
   → maybe </rich_text>
        <rich_text link="webs https://doi.org/10.1109/FMEC.2019.8795362">https://doi.org/10.1109/FMEC.2019.8795362</rich_text>
        <rich_text>
   → maybe </rich_text>
        <rich_text link="webs https://link.springer.com/article/10.1007/s10845-021-01808-w#Sec9">https://link.springer.com/article/10.1007/s10845-021-01808-w#Sec9</rich_text>
        <rich_text>
   → extremely relevant: </rich_text>
        <rich_text link="webs https://doi.org/10.1109/SACI51354.2021.9465588">https://doi.org/10.1109/SACI51354.2021.9465588</rich_text>
        <rich_text>
   → Maybe </rich_text>
        <rich_text link="webs https://doi.org/10.1186/s40537-020-00303-y">https://doi.org/10.1186/s40537-020-00303-y</rich_text>
        <rich_text>
   → maybe </rich_text>
        <rich_text link="webs https://doi.org/10.1080/17460441.2021.1932812">https://doi.org/10.1080/17460441.2021.1932812</rich_text>
        <rich_text>
   → thesis: </rich_text>
        <rich_text link="webs http://hdl.handle.net/10211.3/217176">http://hdl.handle.net/10211.3/217176</rich_text>
        <rich_text>
   → </rich_text>
        <rich_text link="webs https://doi.org/10.1007/978-3-030-50323-9_10">https://doi.org/10.1007/978-3-030-50323-9_10</rich_text>
        <rich_text>
   → maybe </rich_text>
        <rich_text link="webs https://doi.org/10.3390/app10238348">https://doi.org/10.3390/app10238348</rich_text>
        <rich_text>
   → maybe </rich_text>
        <rich_text link="webs https://doi.org/10.1007/978-3-030-70006-5_5">https://doi.org/10.1007/978-3-030-70006-5_5</rich_text>
        <rich_text>
   → </rich_text>
        <rich_text link="webs https://doi.org/10.1007/978-3-030-62365-4_28">https://doi.org/10.1007/978-3-030-62365-4_28</rich_text>
        <rich_text>
   → </rich_text>
        <rich_text link="webs https://doi.org/10.1609/aimag.v41i3.5321">https://doi.org/10.1609/aimag.v41i3.5321</rich_text>
        <rich_text>
   →



machine learning devops
mlops
machine learning deploy
mlops deploy</rich_text>
      </node>
      <node name="11.10.2021" unique_id="19" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1632909308" ts_lastsave="1634544877">
        <rich_text>Next time:
- create table of features and requirements for each devops step
- Try to identify gaps
   → maybe look at discussions in projects utilizing devops for ai, see what people are complaining about.</rich_text>
      </node>
      <node name="18.10.2021" unique_id="26" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634544877" ts_lastsave="1634544986">
        <rich_text>- search strategy:
   → use 1 iteration of snowballing from 20 papers found from systematic mapping study (maybe 30 papers total should be enough)
- extract info about what paper is about
   → achievements
   → challenges
   → etc?
- from challenges extracted, pick one to focus on in thesis.</rich_text>
      </node>
      <node name="26.10.2021" unique_id="32" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1635161521" ts_lastsave="1635238706">
        <rich_text>- keep in back of mind what can be improved while reviewing
   → for master
- description of related work can be short, but need to differentiate my paper from their
- do quality assessment
   → exclude papers with too little information
   → criteria
      ⇒ design quality?
      ⇒ research questions
      ⇒ discussion
   → give points</rich_text>
      </node>
    </node>
    <node name="Review protocol" unique_id="21" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1633172830" ts_lastsave="1635160734">
      <node name="Research questions" unique_id="24" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634027095" ts_lastsave="1634719554">
        <rich_text>Main question: What is the state of the art and state of the practice in ML deployment?

Focusing on automation tooling means that the study is building upon earlier work instead of repeating it.

sub questions:
1. How is ML model deployment handled in real-world applications? (non-lab environments)
2. What are the main challenges and pain points in ML model deployment?
3. What tools are used to deploy ML models?
4. Are there any feature gaps in the tooling used to deploy ML models?

Expected output:
1. Gives an overview of the deployment process and tooling.
2. Identifies what challenges are currently obstructing deployments
3. An overview of what tooling features are used for deploying ML models. What tools are used for CI/CD/CT?
4. Identification of requirements/needs in automated ML deployment that are not covered by current tooling. Basically features that are required to solve challenges in SQ2.


What to include in “deployment”:
- CI/CD-related stuff
- Infrastructure management
- Configuration management
- model/data monitoring
- retraining</rich_text>
      </node>
      <node name="Outputs of earlier work" unique_id="23" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634026500" ts_lastsave="1634043222">
        <rich_text justification="left"></rich_text>
        <rich_text>
*Unpublished work</rich_text>
        <table char_offset="0" justification="left" col_min="97" col_max="97" col_widths="0,0,202,0,0,0,0,0,0,0,0,0">
          <row>
            <cell>Most recent papers included</cell>
            <cell>early 2019</cell>
            <cell>late 2019</cell>
            <cell>2020</cell>
            <cell>mid 2020</cell>
            <cell>mid 2020</cell>
            <cell>early 2020</cell>
            <cell>mid 2020</cell>
            <cell>end of 2020</cell>
            <cell>mid 2020</cell>
            <cell>mid 2021</cell>
            <cell>late 2021</cell>
          </row>
          <row>
            <cell>Method</cell>
            <cell>SLR + semi-structured interviews</cell>
            <cell>SLR</cell>
            <cell>SLR</cell>
            <cell>MLR + questionnaire</cell>
            <cell>MLR</cell>
            <cell>MLR + focus group</cell>
            <cell>SLR</cell>
            <cell>SLR</cell>
            <cell>SLR</cell>
            <cell>SMS</cell>
            <cell>MLR</cell>
          </row>
          <row>
            <cell>Focus</cell>
            <cell>Mostly focuses on challenges. Uses interview results to confirm findings in the literature.
Divides into areas pre-deployment, deployment and non-technical.</cell>
            <cell>Maps SE challenges of ML to the KAs of the SWEBOK</cell>
            <cell>SLR of SE practices for ML, and mapped to KAs in SWEBOK. </cell>
            <cell>Compiles a list of SE best practices for ML.
Performs questionnaire to investigate adoption and effects of practices, as well as demographic relationships.</cell>
            <cell>Synthesizes framework for the development/deployment/evolution process</cell>
            <cell>Proposes conceptual model of five stages of improvement for deploying models.
Mainly considers deployment step of SE process.</cell>
            <cell>Proposes DevOps practices for ML applications.</cell>
            <cell>Maps challenges and proposed solutions to different knowledge areas of SWEBOK.
Considers entirety of SE process.</cell>
            <cell></cell>
            <cell></cell>
            <cell>Tools and gaps in deployment process</cell>
          </row>
          <row>
            <cell>Comments</cell>
            <cell>Emphasizes continuous evaluation of deployed models.</cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell>Poor GL coverage and selection</cell>
            <cell></cell>
            <cell>Decent recommendations.
Research method is only described in a single sentence, poorly documented.</cell>
            <cell>High quality.</cell>
            <cell>Very author-centric approach when describing results.</cell>
            <cell>Very comprehensive and high quality study.
Cites Giray 2021</cell>
            <cell></cell>
          </row>
          <row>
            <cell>                            </cell>
            <cell>Baier et al. 2019</cell>
            <cell>Kumeno 2020</cell>
            <cell>Nascimento et al. 2020*</cell>
            <cell>Serban et al. 2020</cell>
            <cell>John et al. 2021 </cell>
            <cell>Lwakatare et al. 2020</cell>
            <cell>Karamitsos et al. 2020</cell>
            <cell>Giray 2021               </cell>
            <cell>Loranzoni et al. 2021</cell>
            <cell>Martinez-Fernandes et al. 2021*</cell>
            <cell>Me                       </cell>
          </row>
        </table>
      </node>
      <node name="Search strategy" unique_id="20" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1633168889" ts_lastsave="1634561192">
        <rich_text>Earlier search strategies for WL:
- Giray 2021:
   → used search string “Software engineering” AND ("Machine learning" OR “Deep learning”)
   → DB search: ACM, Google Scholar, IEEE, ScienceDirect, Springer, Wiley
      ⇒ Backward and forward snowballing using Google Scholar
   → Manual search:
      ⇒ International Workshop on Artificial Intelligence for Requirements Engineering (AIRE): 2014–2020
      ⇒ International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE): 2012–2020
      ⇒ IEEE/ACM International Conference on Software Engineering: 2017–2020
      ⇒ ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering: 2017–2020
- John et al 2020:
   → search string:
      ⇒ software AND (deploy* OR production) AND (integrat* OR inference OR serv* OR monitor* OR scale OR evol*) AND (edge OR cloud OR hybrid) AND (“machine learning” OR “deep learning”)
   → Libraries: IEEE Xplore, ACM Digital Library, Scopus, ScienceDirect, Web of Science
   → No manual search or snowballing
- Lwakatare 2020:
   → snowballing:
      ⇒ 8 seed papers (starting set) based on previous knowledge + manual search. spans different communities.
      ⇒ one iteration of forward and backward snowballing performed.
   → db search:
      ⇒ string formulated after several pilots:
         • (software AND (engineering OR process OR requirements OR design* OR construction OR testing OR debug* OR deploy* OR maintenance) AND (“machine learning”)
- Nascimento 2020
   → search string:
      ⇒ ("Machine Learning" OR "Data Science" OR "Data-Driven Application" OR "AI application" OR "Artificial Intelligence") AND ("Software Engineering" OR "Software Requirement" OR  "Software Development" OR "System Development" OR "Software Application" OR "Software Architecture" OR "Software Testing") AND ("empirical" OR "case study*" OR "survey" OR "questionnaire" OR "interview*" OR "action research" OR "ethnography" OR "Experience report" OR "Experiment design")

Oria (litteratursøk) covers following DBs:
- ACMDL, Arxiv, Compendex, Dimensions, Gartner, IEEE Xplore, Inspec, Scopus, SpringerLink, Standard.no, Web of Science

should include MLOps in search string


</rich_text>
        <rich_text scale="h2">White literature search strategy</rich_text>
        <rich_text>
Use seed papers as basis for snowballing.
Do one iteration of both forward and backward snowballing.
(Maybe include some papers from manual search)
Use Wohlin 2014 as guideline

</rich_text>
        <rich_text scale="h2">Grey literature search strategy</rich_text>
        <rich_text>
search “mlops deployment” and “machine learning deployment” on google
use top 100 results. Continue if results on last page are still relevant.</rich_text>
      </node>
      <node name="Quality assessment" unique_id="29" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634721665" ts_lastsave="1635160600">
        <rich_text scale="h3">GL quality assessment form</rich_text>
        <rich_text>

Authority of the producer
• Is the publishing organization reputable?
• Is an individual author associated with a reputable organization?
• Has the author published other work in the ﬁeld?
• Does the author have expertise in the area? (e.g. job title principal software engineer)
Methodology
• Does the source have a clearly stated aim?
• Does the source have a stated methodology?
• Is the source supported by authoritative, contemporary references?
• Are any limitations clearly stated?
• Does the work cover a speciﬁc question?
• Does the work refer to a particular population or case?
Objectivity
• Is the statement in the sources as objective as possible? Or, is the statement a subjective opinion?
• Is there vested interest? E.g., a tool comparison by authors that are working for a particular tool vendor
Date
• Does the item have a clearly stated date?
Position w.r.t. related sources
• Have key related GL or formal sources been linked to/discussed?
Novelty
• Does it enrich or add something unique to the research?
• Does it strengthen or refute a current position?
Impact
• Normalize all the following impact metrics into a single aggregated impact metric (when data are available): Number of citations, Number of backlinks, Number of social media shares (the so-called “alt-metrics ”), Number of comments posted for a speciﬁc online entries like a blog post or a video, Number of page or paper views 
Outlet type
• 1st tier GL (measure = 1): High outlet control/ High credibility: Books, magazines, theses, government reports, white papers
• 2nd tier GL (measure = 0.5): Moderate outlet control/ Moderate credibility: Annual reports, news articles, presentations, videos, Q/A sites (such as StackOverﬂow), Wiki articles
• 3rd tier GL (measure = 0): Low outlet control/Low credibility: Blogs, emails, tweets

WL quality assessment form

Methodology
• Does the source have a clearly stated aim?
• Does the source have a stated methodology?
• Are any limitations clearly stated?
• Does the work cover a speciﬁc question?
• Does the work refer to a particular population or case?
Objectivity
• Is there vested interest? E.g., a tool comparison by authors that are working for a particular tool vendor
Position w.r.t. related sources
• Have key related GL or formal sources been linked to/discussed?
Novelty
• Does it enrich or add something unique to the research?
• Does it strengthen or refute a current position?
Impact
• Normalize all the following impact metrics into a single aggregated impact metric (when data are available): Number of citations, Number of page or paper views</rich_text>
      </node>
      <node name="Study selection" unique_id="27" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634558351" ts_lastsave="1635153026">
        <rich_text scale="h2">White literature selection criteria
</rich_text>
        <rich_text>
</rich_text>
        <rich_text scale="h3">WL inclusion criteria</rich_text>
        <rich_text>
- Discusses one of the following aspects of machine learning deployment: challenges, solutions, tooling, processes, requirements
- In english
- Peer reviewed and published in either journal, conference proceedings or workshop
- Published after 2010
- Available online</rich_text>
        <rich_text scale="h2">
</rich_text>
        <rich_text>
</rich_text>
        <rich_text scale="h3">WL exclusion criteria</rich_text>
        <rich_text>
- Does not satisfy all inclusion criteria

</rich_text>
        <rich_text scale="h2">
Grey literature selection criteria</rich_text>
        <rich_text>

</rich_text>
        <rich_text scale="h3">GL inclusion criteria</rich_text>
        <rich_text>
- Mostly the same as WL criteria, but doesn't need to be peer-reviewed

</rich_text>
        <rich_text scale="h3">GL exclusion criteria</rich_text>
        <rich_text>
- Does not satisfy all inclusion criteria



</rich_text>
        <rich_text scale="h2">Study selection procedures</rich_text>
        <rich_text>
- I will perform selection
- Any papers I am uncertain about will be discussed with supervisor</rich_text>
      </node>
      <node name="Data extraction" unique_id="28" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634561214" ts_lastsave="1635237736">
        <rich_text scale="h3">Data extraction form:</rich_text>
        <rich_text>
- Source type (WL/GL)
- publication date
- challenges reported
- solution
   → what challenge is addressed?
   → what is the solution?
   → is the solution only proposed or actually implemented? if so, what was the outcome?
- tools used
   → what tools were used for deployment?
- requirements
   → do the authors state functional/non-functional requirements for deployment?
   → 
- gaps
   → is any technical shortcoming either implicitly or explicitly reported?
   → what is the shortcoming?
   → how was the shortcoming inferred?



</rich_text>
        <rich_text scale="h2">Data extraction process</rich_text>
        <rich_text>
1. for single researcher: get supervisor to perform data extraction on random sample and cross-check results. or do test-retest, i.e. do the data extraction twice for random subset.</rich_text>
      </node>
      <node name="Data synthesis" unique_id="30" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634728737" ts_lastsave="1634729383">
        <rich_text>Will use descriptive synthesis
Tabulate extracted information in a manner consistent with RQs
Tables should be structured to highlight similarities and differences in study outcomes
Should identify whether studies are consistent or inconsistent (homogeneous vs heterogeneous)
Line of argument synthesis

Sensitivity analysis for qualitative synthesis: should consider what impact excluding poor studies or studies of particular type would have on conclusions.</rich_text>
      </node>
      <node name="Sensitivity analysis" unique_id="31" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1635160734" ts_lastsave="1635160769">
        <rich_text>Perform synthesis on various subsets of the literature, see if conclusions are changed drastically.</rich_text>
      </node>
    </node>
  </node>
</cherrytree>
