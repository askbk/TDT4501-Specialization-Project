<?xml version="1.0" encoding="UTF-8"?>
<cherrytree>
  <bookmarks list=""/>
  <node name="Specialization project" unique_id="1" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="1" foreground="" ts_creation="1630387901" ts_lastsave="1633168889">
    <rich_text scale="h1">DevOps for AI</rich_text>
    <rich_text>



</rich_text>
    <rich_text scale="h2">Research question drafts by category:</rich_text>
    <rich_text>
</rich_text>
    <rich_text scale="h3">Architecture</rich_text>
    <rich_text>
- message-based training architecture
</rich_text>
    <rich_text scale="h3">Versioning</rich_text>
    <rich_text>
- How to version online models?
   → Typical architecture for contextual bandit: </rich_text>
    <rich_text link="webs https://youtu.be/um6Sq5EhW6A?t=3117">https://youtu.be/um6Sq5EhW6A?t=3117</rich_text>
    <rich_text>
   → What is the solution for other types of machine learning systems?
- How to version projects? (data + model + code)
   → Git + DVC
   → Pachyderm
- have we deployed the correct model?

</rich_text>
    <rich_text scale="h3">Dependencies</rich_text>
    <rich_text>
- How to find the transitive closure of all data dependencies? (HTDMLS)
- How to deploy model with breaking changes?
   → E.g.: Schema for input is changed
- How to detect changes in data dependencies?
   → Detect changes in data value/schema that production model relies on from external systems
   → Detect changes in system that depend on model
- How to measure the change to the system when a new change is deployed? (HTDMLS)
- How to predict whether improvement to a model will degrade another model in the system? (HTDMLS)

</rich_text>
    <rich_text scale="h3">Monitorability</rich_text>
    <rich_text>
- How to architect for monitorability?
   → see how it is done in devops

</rich_text>
    <rich_text scale="h3">Training, updating and retraining</rich_text>
    <rich_text>
- Model abstraction to decouple training and operating environments
- Model drift: when should we retrain?
- hyperparameter tuning

</rich_text>
    <rich_text scale="h3">Misc</rich_text>
    <rich_text>
- How to handle complex data ownership?
- How to enable language-agnostic MLOps?
- Broad topic: MLOps for online learning systems
   → Governance
   → Model-free reinforcement learning
   → Model versioning
- DevSecOps - how to ensure security and privacy in MLops?
- how to handle adversarial perturbations?
   → how to detect? what to do about it?
- Managing competitive training of model variants against each other in dev environments
   → for online learning models
- Emergency cut out
- how to solve grade-your-own-exam anti-pattern in MLOps context?
- how to communicate model uncertainty?

- how to do deployments in a federated learning context?
- how to build CI/CD pipelines for federated learning?
- monitorability, auditability, etc in federated learning?
- research questions
   → stakeholders
   → impact

- tools for full lifecycle:
   → polyaxon
   → valohai</rich_text>
    <node name="SEMLA webinar notes" unique_id="4" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630390311" ts_lastsave="1631185334">
      <rich_text>- Versioning
   → What model version did i run? Which data was it trained on?
   → Track metadata across systems
- Monitoring model drift
- Alerting
- Logging
- Auditing
- Model versioning
- Heterogeneous systems
- Repeatability, reproducibility
- Abstract infrastructure away
- Continuous integration, deployment
- Composability
- “Hidden technical debt in machine learning systems” - google
- Algorithmia white papers
- “Foundations for ML at Scale” - Peter Skomoroch
- Performance SLAs
- Accounting
- Daily retraining and redeployment with new data
- Infrastructure tasks during training and inference
- Toughtworks “Get MLops right”
- Data changes over time:
   → Schema
   → Sampling over time
   → Volume
- Model changes over time:
   → Algorithms
   → More training
   → Experiments
- Code changes over time:
   → Business need
   → Bug fixes
   → Configuration
- Multiple repos for data, schema, model, container
   → Meta-repo for tracking dependencies between repos?
- Model drift: data drift vs concept drift
- </rich_text>
      <rich_text underline="single" link="webs https://martinfowler.com/articles/cd4ml.html" foreground="#1155cc">https://martinfowler.com/articles/cd4ml.html</rich_text>
      <rich_text>
- Challenges and opportunities for architecting ML systems
   → Architecture patterns and tactics for ML-important quality attributes
   → Monitorability as a driving quality attribute
   → Co-architecting and co-versioning
- Quality attribute metrics for fairness/ethics</rich_text>
    </node>
    <node name="Papers" unique_id="5" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630485213" ts_lastsave="1634115427">
      <node name="Papers to read" unique_id="3" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630389156" ts_lastsave="1631280378">
        <rich_text>AI Engineering - Software Engineering for AI (WAIN):
- </rich_text>
        <rich_text link="webs https://ieeexplore.ieee.org/xpl/conhome/9473903/proceeding">https://ieeexplore.ieee.org/xpl/conhome/9473903/proceeding</rich_text>
        <rich_text>

Overview:
</rich_text>
        <rich_text strikethrough="true">- WHy do 80% blabal: </rich_text>
        <rich_text strikethrough="true" link="webs https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/">https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/</rich_text>
        <rich_text>
</rich_text>
        <rich_text strikethrough="true">- Using AntiPatterns to avoid MLOps Mistakes: </rich_text>
        <rich_text strikethrough="true" link="webs https://arxiv.org/pdf/2107.00079.pdf">https://arxiv.org/pdf/2107.00079.pdf</rich_text>
        <rich_text>
</rich_text>
        <rich_text strikethrough="true">- Who Needs MLOps: What Data Scientists Seek to Accomplish and How Can MLOps Help?: </rich_text>
        <rich_text strikethrough="true" link="webs https://arxiv.org/pdf/2103.08942.pdf">https://arxiv.org/pdf/2103.08942.pdf</rich_text>
        <rich_text>
- </rich_text>
        <rich_text strikethrough="true">MLOps Challenges in Multi-Organization Setup: Experiences from Two Real-World Cases: </rich_text>
        <rich_text strikethrough="true" link="webs https://arxiv.org/pdf/2103.08937.pdf">https://arxiv.org/pdf/2103.08937.pdf</rich_text>
        <rich_text>
- A Data Quality-Driven View of MLOps: </rich_text>
        <rich_text link="webs https://arxiv.org/pdf/2102.07750.pdf">https://arxiv.org/pdf/2102.07750.pdf</rich_text>
        <rich_text>
- The Collective Knowledge project: making ML models more portable and reproducible with open APIs, reusable best practices and MLOps: </rich_text>
        <rich_text link="webs https://arxiv.org/pdf/2006.07161.pdf">https://arxiv.org/pdf/2006.07161.pdf</rich_text>
        <rich_text>
- Software Engineering for Machine Learning: A Case Study: </rich_text>
        <rich_text link="webs https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/">https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/</rich_text>
        <rich_text>
- Challenges in the deployment and operation of machine learning in practice: </rich_text>
        <rich_text link="webs https://www.researchgate.net/profile/Lucas-Baier/publication/332996647_CHALLENGES_IN_THE_DEPLOYMENT_AND_OPERATION_OF_MACHINE_LEARNING_IN_PRACTICE/links/5cd57a7c92851c4eab924c03/CHALLENGES-IN-THE-DEPLOYMENT-AND-OPERATION-OF-MACHINE-LEARNING-IN-PRACTICE.pdf">https://www.researchgate.net/profile/Lucas-Baier/publication/332996647_CHALLENGES_IN_THE_DEPLOYMENT_AND_OPERATION_OF_MACHINE_LEARNING_IN_PRACTICE/links/5cd57a7c92851c4eab924c03/CHALLENGES-IN-THE-DEPLOYMENT-AND-OPERATION-OF-MACHINE-LEARNING-IN-PRACTICE.pdf</rich_text>
        <rich_text>
</rich_text>
        <rich_text strikethrough="true">- Challenges in Deploying Machine Learning: a Survey of Case Studies: </rich_text>
        <rich_text strikethrough="true" link="webs https://arxiv.org/pdf/2011.09926">https://arxiv.org/pdf/2011.09926</rich_text>
        <rich_text>
Monitoring:
- Monitoring machine learning models in production: </rich_text>
        <rich_text link="webs https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/">https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/</rich_text>
        <rich_text>
- Monitoring machine learning models: </rich_text>
        <rich_text link="webs https://towardsdatascience.com/monitoring-machine-learning-models-62d5833c7ecc">https://towardsdatascience.com/monitoring-machine-learning-models-62d5833c7ecc</rich_text>
        <rich_text>
- </rich_text>
        <rich_text link="webs https://ai.google/research/pubs/pub46555">The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction</rich_text>
        <rich_text> (Google)
- Monitoring Azure Machine Learning: </rich_text>
        <rich_text link="webs https://docs.microsoft.com/en-us/azure/machine-learning/monitor-azure-machine-learning">https://docs.microsoft.com/en-us/azure/machine-learning/monitor-azure-machine-learning</rich_text>
        <rich_text>
- Deploying Machine Learning Models for Public Policy: A Framework: </rich_text>
        <rich_text link="webs https://dl.acm.org/doi/pdf/10.1145/3219819.3219911">https://dl.acm.org/doi/pdf/10.1145/3219819.3219911</rich_text>
        <rich_text>

- books:
   → accelerate: the science of lean software
   → the phoenix framework
   → deploying AI in the enterprise: </rich_text>
        <rich_text link="webs https://link.springer.com/book/10.1007%2F978-1-4842-6206-1">https://link.springer.com/book/10.1007%2F978-1-4842-6206-1</rich_text>
        <rich_text>

A lot of papers: </rich_text>
        <rich_text link="webs https://github.com/SE-ML/awesome-seml">https://github.com/SE-ML/awesome-seml</rich_text>
        <rich_text>

On conducting MLRs (28 pages): </rich_text>
        <rich_text link="webs https://arxiv.org/ftp/arxiv/papers/1707/1707.02553.pdf">Guidelines for conducting MLRs-Sept 15-2.docx (arxiv.org)</rich_text>
      </node>
    </node>
    <node name="Useful for thesis" unique_id="12" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1631184972" ts_lastsave="1633605763">
      <rich_text>Federated Learning: Privacy and incentive. (2020). </rich_text>
      <rich_text link="webs https://link.springer.com/book/10.1007%2F978-3-030-63076-8">https://link.springer.com/book/10.1007%2F978-3-030-63076-8</rich_text>
      <rich_text>
</rich_text>
      <rich_text link="webs https://federated.fastforwardlabs.com/">https://federated.fastforwardlabs.com/</rich_text>
      <rich_text>
</rich_text>
      <rich_text link="webs https://federated.withgoogle.com/">https://federated.withgoogle.com/</rich_text>
      <rich_text>
Elixir for programming distributed systems
Lwakatare et al 2020, mentions deployment of federated learning:
	“According to practitioners' experience [P42], federated learning requires specialized deployment frameworks capable to handle changes at the edge.” -

</rich_text>
      <rich_text link="webs https://www.openmined.org/">https://www.openmined.org/</rich_text>
      <rich_text>
</rich_text>
      <rich_text link="webs https://github.com/FederatedAI/FATE">https://github.com/FederatedAI/FATE</rich_text>
      <rich_text>
</rich_text>
      <rich_text link="webs https://github.com/scaleoutsystems/fedn">https://github.com/scaleoutsystems/fedn</rich_text>
    </node>
    <node name="Supervision meetings" unique_id="13" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1631197091" ts_lastsave="1637055783">
      <node name="26.08.2021" unique_id="14" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1631197176" ts_lastsave="1631197290">
        <rich_text>Who will i help? Find out stakeholders
- Data scientist? Software developers? managers?


Find pain point
- Who cares about e.g. deployment time
- Who will care? Why? Propose reasons
- Do literature review. Does anyone know why the problem exists? Find out why.
- Is the reason for the problem known? If not, find out reason first. If known, address the problem directly.
- Is it big problem or trivial?

</rich_text>
        <rich_text weight="heavy">For next meeting:</rich_text>
        <rich_text>
- 4 Research questions
- Detailed analysis
   → Stakeholders
   → Problem
   → Impact



</rich_text>
      </node>
      <node name="10.09.2021 - Discussing research questions" unique_id="15" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1631197290" ts_lastsave="1631280229">
        <rich_text>- Project: Systematic literature review of e.g. CI/CD in MLOps.
   → Look at what systematic literature reviews on CI/CD in MLOps already exist
   → Find out how to stand out so you make a contribution
      ⇒ Include different sets of papers, e.g. industrial if that hasn't been done before

Next time, create plan for systematic literature review.

Read guidelines for conducting multivocal literature reviews: </rich_text>
        <rich_text link="webs https://arxiv.org/ftp/arxiv/papers/1707/1707.02553.pdf">Microsoft Word - Guidelines for conducting MLRs-Sept 15-2.docx (arxiv.org)</rich_text>
        <rich_text>
(Read book in TDT39?)
</rich_text>
      </node>
      <node name="24.09.2021 - Input on research plan" unique_id="18" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1632479208" ts_lastsave="1632909308">
        <rich_text>- make table comparing expected output with output of existing MLR
- semantic analysis step in data synthesis
- oria for querying
   → searches IEEE, ACM, etc
   → filters duplicates
- for next time:
   → do some pilot queries
   → construct selection criteria
   → discuss with bill next time</rich_text>
      </node>
      <node name="5.10.2021" unique_id="22" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1633335688" ts_lastsave="1633961352">
        <rich_text>Lwakatare et al (2020):
- Creates a conceptual model of five stages of ML development process improvement

- Focus on tools for deployment?
   → difficult to perform literature review? academic papers rarely mention tools(?)

Modified research questions:
- What is the state of the art and state of the practice in deploying ML models?
   → SQ1: How are ML models being deployed in real-world applications?
   → SQ2: What tools are used to deploy ML models? focus on features
   → SQ3: What are the main challenges and pain points in deploying ML models?
   → What are the limitations of the tools?

- New study focus provides additional inclusion criteria for study selection:
   → Paper has to mention specific tools used


- Until next time:
   → Do pilot search, find 10 maybe 20 papers related to tools to make sure I am in good shape


- Papers found:
   → </rich_text>
        <rich_text link="webs https://doi.org/10.1109/ICSE-SEIP.2019.00042">https://doi.org/10.1109/ICSE-SEIP.2019.00042</rich_text>
        <rich_text>
   → </rich_text>
        <rich_text link="webs https://doi.org/10.1007/978-3-030-19034-7_14">https://doi.org/10.1007/978-3-030-19034-7_14</rich_text>
        <rich_text>
   → maybe </rich_text>
        <rich_text link="webs https://doi.org/10.1109/FMEC.2019.8795362">https://doi.org/10.1109/FMEC.2019.8795362</rich_text>
        <rich_text>
   → maybe </rich_text>
        <rich_text link="webs https://link.springer.com/article/10.1007/s10845-021-01808-w#Sec9">https://link.springer.com/article/10.1007/s10845-021-01808-w#Sec9</rich_text>
        <rich_text>
   → extremely relevant: </rich_text>
        <rich_text link="webs https://doi.org/10.1109/SACI51354.2021.9465588">https://doi.org/10.1109/SACI51354.2021.9465588</rich_text>
        <rich_text>
   → Maybe </rich_text>
        <rich_text link="webs https://doi.org/10.1186/s40537-020-00303-y">https://doi.org/10.1186/s40537-020-00303-y</rich_text>
        <rich_text>
   → maybe </rich_text>
        <rich_text link="webs https://doi.org/10.1080/17460441.2021.1932812">https://doi.org/10.1080/17460441.2021.1932812</rich_text>
        <rich_text>
   → thesis: </rich_text>
        <rich_text link="webs http://hdl.handle.net/10211.3/217176">http://hdl.handle.net/10211.3/217176</rich_text>
        <rich_text>
   → </rich_text>
        <rich_text link="webs https://doi.org/10.1007/978-3-030-50323-9_10">https://doi.org/10.1007/978-3-030-50323-9_10</rich_text>
        <rich_text>
   → maybe </rich_text>
        <rich_text link="webs https://doi.org/10.3390/app10238348">https://doi.org/10.3390/app10238348</rich_text>
        <rich_text>
   → maybe </rich_text>
        <rich_text link="webs https://doi.org/10.1007/978-3-030-70006-5_5">https://doi.org/10.1007/978-3-030-70006-5_5</rich_text>
        <rich_text>
   → </rich_text>
        <rich_text link="webs https://doi.org/10.1007/978-3-030-62365-4_28">https://doi.org/10.1007/978-3-030-62365-4_28</rich_text>
        <rich_text>
   → </rich_text>
        <rich_text link="webs https://doi.org/10.1609/aimag.v41i3.5321">https://doi.org/10.1609/aimag.v41i3.5321</rich_text>
        <rich_text>
   →



machine learning devops
mlops
machine learning deploy
mlops deploy</rich_text>
      </node>
      <node name="11.10.2021" unique_id="19" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1632909308" ts_lastsave="1634544877">
        <rich_text>Next time:
- create table of features and requirements for each devops step
- Try to identify gaps
   → maybe look at discussions in projects utilizing devops for ai, see what people are complaining about.</rich_text>
      </node>
      <node name="18.10.2021" unique_id="26" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634544877" ts_lastsave="1634544986">
        <rich_text>- search strategy:
   → use 1 iteration of snowballing from 20 papers found from systematic mapping study (maybe 30 papers total should be enough)
- extract info about what paper is about
   → achievements
   → challenges
   → etc?
- from challenges extracted, pick one to focus on in thesis.</rich_text>
      </node>
      <node name="26.10.2021" unique_id="32" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1635161521" ts_lastsave="1635238706">
        <rich_text>- keep in back of mind what can be improved while reviewing
   → for master
- description of related work can be short, but need to differentiate my paper from their
- do quality assessment
   → exclude papers with too little information
   → criteria
      ⇒ design quality?
      ⇒ research questions
      ⇒ discussion
   → give points</rich_text>
      </node>
      <node name="02.11.2021" unique_id="33" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1635844993" ts_lastsave="1635845026">
        <rich_text>- Comparison table with related work should explicitly argue uniqueness, not scope</rich_text>
      </node>
      <node name="16.11.2021" unique_id="34" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1637055783" ts_lastsave="1637055798">
        <rich_text>- Next time: we should discuss comparison table
   → Have table ready</rich_text>
      </node>
    </node>
    <node name="Review protocol" unique_id="21" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1633172830" ts_lastsave="1635160734">
      <node name="Research questions" unique_id="24" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634027095" ts_lastsave="1634719554">
        <rich_text>Main question: What is the state of the art and state of the practice in ML deployment?

Focusing on automation tooling means that the study is building upon earlier work instead of repeating it.

sub questions:
1. How is ML model deployment handled in real-world applications? (non-lab environments)
2. What are the main challenges and pain points in ML model deployment?
3. What tools are used to deploy ML models?
4. Are there any feature gaps in the tooling used to deploy ML models?

Expected output:
1. Gives an overview of the deployment process and tooling.
2. Identifies what challenges are currently obstructing deployments
3. An overview of what tooling features are used for deploying ML models. What tools are used for CI/CD/CT?
4. Identification of requirements/needs in automated ML deployment that are not covered by current tooling. Basically features that are required to solve challenges in SQ2.


What to include in “deployment”:
- CI/CD-related stuff
- Infrastructure management
- Configuration management
- model/data monitoring
- retraining</rich_text>
      </node>
      <node name="Outputs of earlier work" unique_id="23" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634026500" ts_lastsave="1637240604"/>
      <node name="Search strategy" unique_id="20" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1633168889" ts_lastsave="1634561192">
        <rich_text>Earlier search strategies for WL:
- Giray 2021:
   → used search string “Software engineering” AND ("Machine learning" OR “Deep learning”)
   → DB search: ACM, Google Scholar, IEEE, ScienceDirect, Springer, Wiley
      ⇒ Backward and forward snowballing using Google Scholar
   → Manual search:
      ⇒ International Workshop on Artificial Intelligence for Requirements Engineering (AIRE): 2014–2020
      ⇒ International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE): 2012–2020
      ⇒ IEEE/ACM International Conference on Software Engineering: 2017–2020
      ⇒ ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering: 2017–2020
- John et al 2020:
   → search string:
      ⇒ software AND (deploy* OR production) AND (integrat* OR inference OR serv* OR monitor* OR scale OR evol*) AND (edge OR cloud OR hybrid) AND (“machine learning” OR “deep learning”)
   → Libraries: IEEE Xplore, ACM Digital Library, Scopus, ScienceDirect, Web of Science
   → No manual search or snowballing
- Lwakatare 2020:
   → snowballing:
      ⇒ 8 seed papers (starting set) based on previous knowledge + manual search. spans different communities.
      ⇒ one iteration of forward and backward snowballing performed.
   → db search:
      ⇒ string formulated after several pilots:
         • (software AND (engineering OR process OR requirements OR design* OR construction OR testing OR debug* OR deploy* OR maintenance) AND (“machine learning”)
- Nascimento 2020
   → search string:
      ⇒ ("Machine Learning" OR "Data Science" OR "Data-Driven Application" OR "AI application" OR "Artificial Intelligence") AND ("Software Engineering" OR "Software Requirement" OR  "Software Development" OR "System Development" OR "Software Application" OR "Software Architecture" OR "Software Testing") AND ("empirical" OR "case study*" OR "survey" OR "questionnaire" OR "interview*" OR "action research" OR "ethnography" OR "Experience report" OR "Experiment design")

Oria (litteratursøk) covers following DBs:
- ACMDL, Arxiv, Compendex, Dimensions, Gartner, IEEE Xplore, Inspec, Scopus, SpringerLink, Standard.no, Web of Science

should include MLOps in search string


</rich_text>
        <rich_text scale="h2">White literature search strategy</rich_text>
        <rich_text>
Use seed papers as basis for snowballing.
Do one iteration of both forward and backward snowballing.
(Maybe include some papers from manual search)
Use Wohlin 2014 as guideline

</rich_text>
        <rich_text scale="h2">Grey literature search strategy</rich_text>
        <rich_text>
search “mlops deployment” and “machine learning deployment” on google
use top 100 results. Continue if results on last page are still relevant.</rich_text>
      </node>
      <node name="Quality assessment" unique_id="29" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634721665" ts_lastsave="1635160600">
        <rich_text scale="h3">GL quality assessment form</rich_text>
        <rich_text>

Authority of the producer
• Is the publishing organization reputable?
• Is an individual author associated with a reputable organization?
• Has the author published other work in the ﬁeld?
• Does the author have expertise in the area? (e.g. job title principal software engineer)
Methodology
• Does the source have a clearly stated aim?
• Does the source have a stated methodology?
• Is the source supported by authoritative, contemporary references?
• Are any limitations clearly stated?
• Does the work cover a speciﬁc question?
• Does the work refer to a particular population or case?
Objectivity
• Is the statement in the sources as objective as possible? Or, is the statement a subjective opinion?
• Is there vested interest? E.g., a tool comparison by authors that are working for a particular tool vendor
Date
• Does the item have a clearly stated date?
Position w.r.t. related sources
• Have key related GL or formal sources been linked to/discussed?
Novelty
• Does it enrich or add something unique to the research?
• Does it strengthen or refute a current position?
Impact
• Normalize all the following impact metrics into a single aggregated impact metric (when data are available): Number of citations, Number of backlinks, Number of social media shares (the so-called “alt-metrics ”), Number of comments posted for a speciﬁc online entries like a blog post or a video, Number of page or paper views 
Outlet type
• 1st tier GL (measure = 1): High outlet control/ High credibility: Books, magazines, theses, government reports, white papers
• 2nd tier GL (measure = 0.5): Moderate outlet control/ Moderate credibility: Annual reports, news articles, presentations, videos, Q/A sites (such as StackOverﬂow), Wiki articles
• 3rd tier GL (measure = 0): Low outlet control/Low credibility: Blogs, emails, tweets

WL quality assessment form

Methodology
• Does the source have a clearly stated aim?
• Does the source have a stated methodology?
• Are any limitations clearly stated?
• Does the work cover a speciﬁc question?
• Does the work refer to a particular population or case?
Objectivity
• Is there vested interest? E.g., a tool comparison by authors that are working for a particular tool vendor
Position w.r.t. related sources
• Have key related GL or formal sources been linked to/discussed?
Novelty
• Does it enrich or add something unique to the research?
• Does it strengthen or refute a current position?
Impact
• Normalize all the following impact metrics into a single aggregated impact metric (when data are available): Number of citations, Number of page or paper views</rich_text>
      </node>
      <node name="Study selection" unique_id="27" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634558351" ts_lastsave="1635153026">
        <rich_text scale="h2">White literature selection criteria
</rich_text>
        <rich_text>
</rich_text>
        <rich_text scale="h3">WL inclusion criteria</rich_text>
        <rich_text>
- Discusses one of the following aspects of machine learning deployment: challenges, solutions, tooling, processes, requirements
- In english
- Peer reviewed and published in either journal, conference proceedings or workshop
- Published after 2010
- Available online</rich_text>
        <rich_text scale="h2">
</rich_text>
        <rich_text>
</rich_text>
        <rich_text scale="h3">WL exclusion criteria</rich_text>
        <rich_text>
- Does not satisfy all inclusion criteria

</rich_text>
        <rich_text scale="h2">
Grey literature selection criteria</rich_text>
        <rich_text>

</rich_text>
        <rich_text scale="h3">GL inclusion criteria</rich_text>
        <rich_text>
- Mostly the same as WL criteria, but doesn't need to be peer-reviewed

</rich_text>
        <rich_text scale="h3">GL exclusion criteria</rich_text>
        <rich_text>
- Does not satisfy all inclusion criteria



</rich_text>
        <rich_text scale="h2">Study selection procedures</rich_text>
        <rich_text>
- I will perform selection
- Any papers I am uncertain about will be discussed with supervisor</rich_text>
      </node>
      <node name="Data extraction" unique_id="28" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634561214" ts_lastsave="1635237736">
        <rich_text scale="h3">Data extraction form:</rich_text>
        <rich_text>
- Source type (WL/GL)
- publication date
- challenges reported
- solution
   → what challenge is addressed?
   → what is the solution?
   → is the solution only proposed or actually implemented? if so, what was the outcome?
- tools used
   → what tools were used for deployment?
- requirements
   → do the authors state functional/non-functional requirements for deployment?
   → 
- gaps
   → is any technical shortcoming either implicitly or explicitly reported?
   → what is the shortcoming?
   → how was the shortcoming inferred?



</rich_text>
        <rich_text scale="h2">Data extraction process</rich_text>
        <rich_text>
1. for single researcher: get supervisor to perform data extraction on random sample and cross-check results. or do test-retest, i.e. do the data extraction twice for random subset.</rich_text>
      </node>
      <node name="Data synthesis" unique_id="30" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1634728737" ts_lastsave="1634729383">
        <rich_text>Will use descriptive synthesis
Tabulate extracted information in a manner consistent with RQs
Tables should be structured to highlight similarities and differences in study outcomes
Should identify whether studies are consistent or inconsistent (homogeneous vs heterogeneous)
Line of argument synthesis

Sensitivity analysis for qualitative synthesis: should consider what impact excluding poor studies or studies of particular type would have on conclusions.</rich_text>
      </node>
      <node name="Sensitivity analysis" unique_id="31" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1635160734" ts_lastsave="1635160769">
        <rich_text>Perform synthesis on various subsets of the literature, see if conclusions are changed drastically.</rich_text>
      </node>
    </node>
  </node>
</cherrytree>
