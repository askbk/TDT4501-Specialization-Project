<?xml version="1.0" encoding="UTF-8"?>
<cherrytree>
  <bookmarks list=""/>
  <node name="Specialization project" unique_id="1" prog_lang="custom-colors" tags="" readonly="0" custom_icon_id="0" is_bold="1" foreground="" ts_creation="1630387901" ts_lastsave="1630401209">
    <rich_text scale="h1">DevOps for AI</rich_text>
    <rich_text>

- How to version online models?
   → Typical architecture for contextual bandit: </rich_text>
    <rich_text link="webs https://youtu.be/um6Sq5EhW6A?t=3117">https://youtu.be/um6Sq5EhW6A?t=3117</rich_text>
    <rich_text>
   → What is the solution for other types of machine learning systems?
- How to version projects? (data + model + code)
   → Git + DVC seems most reasonable
   → 
- How to measure the change to the system when a new change is deployed? (HTDMLS)
- How to predict whether improvement to a model will degrade another model in the system? (HTDMLS)
- How to find the transitive closure of all data dependencies? (HTDMLS)
- How to architect for monitorability?
   → see how it is done in devops
- 
- How to use monitorability to increase trust
- How to use devops/mlops tools/principles to increase customer trust in AI decisions
- DevSecOps - how to ensure security and privacy in MLops?
- how to handle adversarial perturbations?
   → how to detect? what to do about it?
- Model drift: when should we retrain?
   → 

- research questions
   → stakeholders
   → impact</rich_text>
    <node name="Hidden technical debt in ML systems" unique_id="2" prog_lang="custom-colors" tags="" readonly="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630388396" ts_lastsave="1630388402">
      <rich_text underline="single" link="webs https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf" foreground="#1155cc">Hidden Technical Debt in Machine Learning Systems</rich_text>
      <rich_text>
Discusses various types of technical debt specific to (or especially prominent in) ML systems:
• Complex models erode boundaries

◇ Entanglement

▪ Changing anything changes everything


◇ Correction cascades

▪ Model interdependency


◇ Undeclared consumers (visibility debt)

▪ Don’t know about consumers of data



• Data dependencies cost more than code dependencies

◇ Unstable data dependencies

◇ Underutilized data dependencies


• Feedback loops

◇ Direct feedback loops

◇ Hidden feedback loops


• ML-system antipatterns

◇ Glue code

◇ Pipeline jungles

◇ Dead experimental code paths

◇ Abstraction debt

◇ Common smells:

▪ Plain-Old-Data type smell

▪ Multiple-language smell

▪ Prototype smell



• Configuration debt

• Dealing with changes in the external world

◇ Fixed thresholds in dynamic systems

◇ Monitoring and testing

▪ Prediction bias

▪ Action limits

▪ Up-stream producers



• Other areas of ml-related debt:

◇ Data testing debt

◇ Reproducibility debt

◇ Process management debt

◇ Cultural debt



</rich_text>
    </node>
    <node name="Papers to read" unique_id="3" prog_lang="custom-colors" tags="" readonly="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630389156" ts_lastsave="1630398881">
      <rich_text>- Using AntiPatterns to avoid MLOps Mistakes: </rich_text>
      <rich_text link="webs https://arxiv.org/pdf/2107.00079.pdf">https://arxiv.org/pdf/2107.00079.pdf</rich_text>
      <rich_text>
- Who Needs MLOps: What Data Scientists Seek to Accomplish and How Can MLOps Help?: </rich_text>
      <rich_text link="webs https://arxiv.org/pdf/2103.08942.pdf">https://arxiv.org/pdf/2103.08942.pdf</rich_text>
      <rich_text>
- MLOps Challenges in Multi-Organization Setup: Experiences from Two Real-World Cases: </rich_text>
      <rich_text link="webs https://arxiv.org/pdf/2103.08937.pdf">https://arxiv.org/pdf/2103.08937.pdf</rich_text>
      <rich_text>
- A Data Quality-Driven View of MLOps: </rich_text>
      <rich_text link="webs https://arxiv.org/pdf/2102.07750.pdf">https://arxiv.org/pdf/2102.07750.pdf</rich_text>
      <rich_text>
- The Collective Knowledge project: making ML models more portable and reproducible with open APIs, reusable best practices and MLOps: </rich_text>
      <rich_text link="webs https://arxiv.org/pdf/2006.07161.pdf">https://arxiv.org/pdf/2006.07161.pdf</rich_text>
      <rich_text>
- Software Engineering for Machine Learning: A Case Study: </rich_text>
      <rich_text link="webs https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/">https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/</rich_text>
      <rich_text>
- Challenges </rich_text>
      <rich_text link="webs https://arxiv.org/pdf/2011.09926">https://arxiv.org/pdf/2011.09926</rich_text>
      <rich_text>
- accelerate: the science of lean software
- the phoenix framework

A lot of papers: </rich_text>
      <rich_text link="webs https://github.com/SE-ML/awesome-seml">https://github.com/SE-ML/awesome-seml</rich_text>
      <rich_text>
</rich_text>
    </node>
    <node name="SEMLA webinar notes" unique_id="4" prog_lang="custom-colors" tags="" readonly="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1630390311" ts_lastsave="1630390326">
      <rich_text>• Versioning
   ◇ What model version did i run? Which data was it trained on?
   ◇ Track metadata across systems

◇ Monitoring model drift

◇ Alerting

◇ Logging

◇ Auditing

◇ Model versioning

◇ Heterogeneous systems

◇ Repeatability, reproducibility

◇ Abstract infrastructure away

◇ Continuous integration, deployment

◇ Composability

◇ “Hidden technical debt in machine learning systems” - google

◇ Algorithmia white papers

◇ “Foundations for ML at Scale” - Peter Skomoroch

◇ Performance SLAs

◇ Accounting

◇ Daily retraining and redeployment with new data

◇ Infrastructure tasks during training and inference

◇ Toughtworks “Get MLops right”

◇ Data changes over time:

▪ Schema

▪ Sampling over time

▪ Volume


◇ Model changes over time:

▪ Algorithms

▪ More training

▪ Experiments


◇ Code changes over time:

▪ Business need

▪ Bug fixes

▪ Configuration


◇ Multiple repos for data, schema, model, container

▪ Meta-repo for tracking dependencies between repos?


◇ Model drift: data drift vs problem drift

◇ </rich_text>
      <rich_text underline="single" link="webs https://martinfowler.com/articles/cd4ml.html" foreground="#1155cc">https://martinfowler.com/articles/cd4ml.html</rich_text>
      <rich_text>

◇ Challenges and opportunities for architecting ML systems

▪ Architecture patterns and tactics for ML-important quality attributes

▪ Monitorability as a driving quality attribute

▪ Co-architecting and co-versioning

◇ Quality attribute metrics for fairness/ethics</rich_text>
    </node>
  </node>
</cherrytree>
