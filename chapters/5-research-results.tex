\chapter{Research Results}
This chapter presents the results from the SLR on a per-research question basis.

\section{RQ1: How is ML Model Deployment Handled in the State of the Art?}
\subsection{Packaging and Integration}
Packaging models in containers accessible through REST or RPC interfaces is a commonly reported method of integration, in which the model is wrapped in an API layer \cite{Garcia2020, Li2017, Ruf2021, Crankshaw2017}.
When deploying many models, using a consistent standard for APIs, such as OpenAPI\footnote{\url{https://www.openapis.org/}}, across models can facilitate system integration \cite{Garcia2020}.
Another reported method of integration is to serialize the model (e.g. into ONNX\footnote{\url{https://onnx.ai/}}, HDF5, joblib) and load it at runtime, possibly with an ML framework that is optimized for production (such as caffe2) \cite{Hazelwood2018, Peticolas2019, Paeaekkoenen2020, Chahal2020}.
The model could also be packaged in a format specific to an end-to-end framework such as MLflow \cite{Chen2020}.
The model may be integrated directly into the application code \cite{Liu2020, Ruf2021, Granlund2021}, possibly first having to be rewritten for production \cite{Hazelwood2018}.

\subsection{Deployment}
Deployment, the transitioning of a  packaged and integrated model into a serving state, may occur through a few different methods.
Models packaged in containers are simply run directly as standalone services \cite{Liu2020,Ruf2021, Granlund2021, Li2017, Garcia2020}.
However, models may be deployed in a target environment that is different from where they are packaged, in which case the model transfer may happen through either a push- or a pull-pattern.

In a pull-pattern deployment, the target environment (host application running on e.g. server or edge device) either periodically polls for model updates and downloads when available \cite{Paeaekkoenen2020, Li2017, Peticolas2019}.

In a push-pattern deployment, the target environment is notified of the availability of a new model by a master server, e.g. by the server where the model was trained.
This will happen either through a messaging service \cite{Liu2020, Garcia2020}, where message contains metadata including the location of the updated model, or by the model being pushed directly to the target environment through an interface \cite{Paeaekkoenen2020}.

When an updated model has reached the target environment, the redeployment can happen in a few different ways.
For models packaged as standalone containers, a container orchestration service (e.g. Kubernetes) can roll out updated models without downtime.
Serialized models may simply be loaded into memory by the application, potentially leading to downtime.
If the hosting application is containerized, model redeployment may be orchestrated by deploying a new application instance alongside the old instance and providing the updated model data.
When the new instance is finished initializing, it can start serving requests, and the old instance may be evicted \cite{Paeaekkoenen2020}.
To avoid high response latencies for the first query (cold-start issues), models may be queried with an empty request (or an explicit warm-up function call may be used if available) which forces lazy-loaded model components to initialize \cite{Li2017}.

When using an ML deployment service (e.g. SageMaker), the service may handle the deployment of serialized models \cite{Chahal2020}.

Models intended for batch predictions may simply be plugged into a computing pipeline such as Apache Spark\footnote{\url{https://spark.apache.org/}} \cite{Li2017}.

\subsection{Serving and Inference}
Models are commonly made available for serving predictions through a REST \cite{Krishnamurthi2019, Liu2020, Ruf2021, Garcia2020, Crankshaw2017, Paeaekkoenen2020} or RPC \cite{Ruf2021, Li2017, Crankshaw2017} API.
To meet inference SLOs, there are several techniques reported.
One is model-switching, where less accurate but more performant models are used during periods of high load in order to meet required SLOs \cite{Zhang2020}.
Another is to use adaptive batching queues with a timeout, where queries are batched together in batch sizes that are tuned to individual the individual model and framework. During periods of low traffic, the timeout is reached before the query batch is filled, and inference is run on the batch in order to not exceed the SLO.
Additionally, a cache layer may be put on top of the model to reduce computation \cite{Crankshaw2017}.
In order to avoid cold-start issues caused by model loading and initialization, models should be warmed up at deployment and be kept perpetually warm over the course of its life \cite{Zhang2020}.
Model warmup can be achieved through either a method provided by the ML framework \cite{Li2017}, or more generally through issuing an empty query against the model \cite{Garcia2020}.

\subsection{Monitoring and Logging}
When the model is operational, the whole application stack is constantly monitored for performance metrics such as latency, throughput, disk utilization, et cetera \cite{Ruf2021, Peticolas2019}.
Predictions are logged, and when available joined with actual outcomes\cite{Li2017}.
Model accuracy should be used to determine when a new model is needed \cite{Peticolas2019}.
The application should be validated against predefined KPIs of the project \cite{Ruf2021}.

\section{RQ2: What Are the Main Challenges in ML Model Deployment?}
\subsection{Serving}
Serving-related challenges are some of the most often reported ones in the literature \cite{Bosch2021, Richins2020, Li2017, Baylor2017, Garcia2020, Crankshaw2017, Lwakatare2019, Bernardi2019, Yadwadkar2019, Chahal2020, Choi2021, Zhang2020, Gupta2020}, usually from a performance perspective.

Achieving a low inference latency and high throughput is widely reported as a challenge in serving ML models \cite{Bosch2021, Li2017,Crankshaw2017,Lwakatare2019,Bernardi2019}.
Several potential solutions have been proposed in the literature.

\textcite{Gupta2020} proposes deploying ML models to the edge in order to reduce network latency. However, edge deployment bring a whole host of its own challenges, which will be discussed in \cref{ch:results:rq2:edge}.

Start-up/cold-start latency is a typical serving challenge for ML models \cite{Yadwadkar2019}. It is mainly caused by the ML model being loaded into memory and initialized, and is typically solved by warming up the model after it has been deployed, and then keeping it perpetually warm \cite{Zhang2020}.

\subsection{Edge}
\label{ch:results:rq2:edge}

\subsection{Monitoring and Logging}
Monitoring ML models is challenging \cite{Li2017, Bernardi2019}, but is required in order to detect performance degradations and build confidence in the model \cite{Bosch2021}.
After making an inference, the true label may not be available for an extended period of time, making it difficult to monitor prediction quality \cite{Bernardi2019}.
One solution used in practice is to look at the distribution of predictions and determine if there is a significant deviation from what an ideal model would look like \cite{Bernardi2019}.

Monitoring models deployed at scale on the edge has another set of associated challenges.
Detecting concept drift usually requires realtime access to the original training data, which may not be possible on edge devices due to network constraints.
Additionally, having a large number of edge devices may introduce scalability challenges \cite{Rausch2019} in monitoring.

\section{RQ3: What Tools and Infrastructure Are Used to Deploy ML Models?}
\section{RQ4: What Are the Gaps in Current Tooling and Infrastructure Used to Deploy ML Models?}