\chapter{Research Results}
This chapter presents the results from the SLR on a per-research question basis.

\section{RQ1: How is ML Model Deployment Handled in the State of the Art?}
\subsection{Packaging and Integration}
Packaging models in containers accessible through REST or RPC interfaces is a commonly reported method of integration, in which the model is wrapped in an API layer \cite{Garcia2020, Li2017, Ruf2021, Crankshaw2017}.
When deploying many models, using a consistent standard for APIs, such as OpenAPI\footnote{\url{https://www.openapis.org/}}, across models can facilitate system integration \cite{Garcia2020}.
Another reported method of integration is to serialize the model (e.g. into ONNX\footnote{\url{https://onnx.ai/}}, HDF5, joblib) and load it at runtime, possibly with an ML framework that is optimized for production (such as caffe2) \cite{Hazelwood2018, Peticolas2019, Paeaekkoenen2020, Chahal2020}.
The model could also be packaged in a format specific to an end-to-end framework such as MLflow \cite{Chen2020}.
The model may be integrated directly into the application code \cite{Liu2020, Ruf2021, Granlund2021}, possibly first having to be rewritten for production \cite{Hazelwood2018}.

\subsection{Deployment}
Deployment, the transitioning of a  packaged and integrated model into a serving state, may occur through a few different methods.
Models packaged in containers are simply run directly as standalone services \cite{Liu2020,Ruf2021, Granlund2021, Li2017, Garcia2020}.
However, models may be deployed in a target environment that is different from where they are packaged, in which case the model transfer may happen through either a push- or a pull-pattern.

In a pull-pattern deployment, the target environment (host application running on e.g. server or edge device) either periodically polls for model updates and downloads when available \cite{Paeaekkoenen2020, Li2017, Peticolas2019}.

In a push-pattern deployment, the target environment is notified of the availability of a new model by a master server, e.g. by the server where the model was trained.
This will happen either through a messaging service \cite{Liu2020, Garcia2020}, where message contains metadata including the location of the updated model, or by the model being pushed directly to the target environment through an interface \cite{Paeaekkoenen2020}.

When an updated model has reached the target environment, the redeployment can happen in a few different ways.
For models packaged as standalone containers, a container orchestration service (e.g. Kubernetes) can roll out updated models without downtime.
Serialized models may simply be loaded into memory by the application, potentially leading to downtime.
If the hosting application is containerized, model redeployment may be orchestrated by deploying a new application instance alongside the old instance and providing the updated model data.
When the new instance is finished initializing, it can start serving requests, and the old instance may be evicted \cite{Paeaekkoenen2020}.
To avoid high response latencies for the first query (cold-start issues), models may be queried with an empty request (or an explicit warm-up function call may be used if available) which forces lazy-loaded model components to initialize \cite{Li2017}.

When using an ML deployment service (e.g. SageMaker), the service may handle the deployment of serialized models \cite{Chahal2020}.

Models intended for batch predictions may simply be plugged into a computing pipeline such as Apache Spark\footnote{\url{https://spark.apache.org/}} \cite{Li2017}.

\subsection{Serving and Inference}
Models are commonly made available for serving predictions through a REST \cite{Krishnamurthi2019, Liu2020, Ruf2021, Garcia2020, Crankshaw2017, Paeaekkoenen2020} or RPC \cite{Ruf2021, Li2017, Crankshaw2017} API.
To meet inference SLOs, there are several techniques reported.
One is model-switching, where less accurate but more performant models are used during periods of high load in order to meet required SLOs \cite{Zhang2020}.
Another is to use adaptive batching queues with a timeout, where queries are batched together in batch sizes that are tuned to individual the individual model and framework. During periods of low traffic, the timeout is reached before the query batch is filled, and inference is run on the batch in order to not exceed the SLO.
Additionally, a cache layer may be put on top of the model to reduce computation \cite{Crankshaw2017}.
In order to avoid cold-start issues caused by model loading and initialization, models should be warmed up at deployment and be kept perpetually warm over the course of its life \cite{Zhang2020}.
Model warmup can be achieved through either a method provided by the ML framework \cite{Li2017}, or more generally through issuing an empty query against the model \cite{Garcia2020}.

\subsection{Monitoring and Logging}
When the model is operational, the whole application stack is constantly monitored for performance metrics such as latency, throughput, disk utilization, et cetera \cite{Ruf2021, Peticolas2019}.
Predictions are logged, and when available joined with actual outcomes\cite{Li2017}.
Model accuracy should be used to determine when a new model is needed \cite{Peticolas2019}.
The application should be validated against predefined KPIs of the project \cite{Ruf2021}.

\section{RQ2: What Are the Main Challenges in ML Model Deployment?}
\subsection{Serving}
Serving-related challenges are some of the most often reported ones in the literature \cite{Bosch2021, Richins2020, Li2017, Baylor2017, Garcia2020, Crankshaw2017, Lwakatare2019, Bernardi2019, Yadwadkar2019, Chahal2020, Choi2021, Zhang2020, Gupta2020}, usually from a performance perspective.

Achieving a low inference latency and high throughput is widely reported as a challenge in serving ML models \cite{Bosch2021, Li2017,Crankshaw2017,Lwakatare2019,Bernardi2019}.
Several potential solutions have been proposed in the literature.

\textcite{Gupta2020} proposes deploying ML models to the edge in order to reduce network latency. However, edge deployment bring a whole host of its own challenges, which will be discussed in \cref{ch:results:rq2:edge}.

Start-up/cold-start latency is a typical serving challenge for ML models \cite{Yadwadkar2019}. It is mainly caused by the ML model being loaded into memory and initialized, and is typically solved by warming up the model after it has been deployed, and then keeping it perpetually warm \cite{Zhang2020}.

\subsection{Edge}
\label{ch:results:rq2:edge}

\subsection{Monitoring and Logging}
Monitoring ML models is challenging \cite{Li2017, Bernardi2019}, but is required in order to detect performance degradations and build confidence in the model \cite{Bosch2021}.
After making an inference, the true label may not be available for an extended period of time, making it difficult to monitor prediction quality \cite{Bernardi2019}.
One solution used in practice is to look at the distribution of predictions and determine if there is a significant deviation from what an ideal model would look like \cite{Bernardi2019}.

Monitoring models deployed at scale on the edge has another set of associated challenges.
Detecting concept drift usually requires realtime access to the original training data, which may not be possible on edge devices due to network constraints.
Additionally, having a large number of edge devices may introduce scalability challenges \cite{Rausch2019} in monitoring.

\section{RQ3: What Tools and Infrastructure Are Used to Deploy ML Models?}
\section{RQ4: What Are the Gaps in Current Tooling and Infrastructure Used to Deploy ML Models?}

Many opportunities for further work and research have been identified in the reviewed literature.
In particular, \cite{Yadwadkar2019} and \cite{Zhang2020} identify a large number of research directions in the area of model serving performance.
\cite{Bosch2021}, \cite{Rausch2019} and \cite{Gupta2020} also propose several areas of research for edge ML deployment.

In the context of an ML system with multiple available variants of the same model (with different accuracy and performance characteristics) and multiple hardware resources to choose from, \cite{Yadwadkar2019} presents a series of interrelated open problems concerned with serving performance.
The automatic selection of model variant based on a given SLO will be referred to as \textit{model-switching} based on the terminology used in \cite{Zhang2020}. 

The overarching question is that of how the serving system should automatically select a model variant and underlying hardware given some SLO specified by a client application.

First, given an inference query, how should it be placed/scheduled by the serving system?
The options are to either generate a new model variant, to load an existing model variant, or to query an already loaded model.
The choice will depend on the individual application requirements, e.g. whether the inference is online or offline, what the SLOs are, etc.
\cite{Yadwadkar2019} and \cite{Zhang2020} propose that further research be conducted into how to synergistically perform model-switching and hardware resource management.

Further, given the choice of placement, a new model may need to be loaded, additional hardware resources may need to be launched, or loaded models may need to be evicted to free up resources.
As \cite{Zhang2020} points out, keeping all models warm at all times is prohibitively expensive and scales poorly.
The model management operations add latency to the query response, affecting the system's ability to meet SLOs.
\cite{Yadwadkar2019} and \cite{Zhang2020} encourage research into methods for reducing or avoiding this latency, striving for the performance of perpetually warm models, while aiming for the resource usage of cold models.

Next, the question is how to capture and organize the data needed for making a decision on query placement.
In addition to the set of possible hardware resources and model variants, the serving system must also take into account the current state of the system, which is continually changing.
The current state of the system is described by which model variants are loaded on which hardware resources, which model variants exist but are not loaded, which model variants do not exist but are able to be generated, CPU/memory/disk/network usage of each hardware instance, inference request traffic.
All of this data needs to be captured and organized in a way that enables fast and efficient decision making on query placement.

Not only does the logical aspect of hardware and model variant selection need to be considered, the geographical location of hardware resources must also be taken into account.
Given that resources may be anywhere on the continuum of core-middle-edge, \cite{Yadwadkar2019} requests research into answering the questions of where models should be loaded, where resource management decisions should be made, and where query placement decisions should be made.
\cite{Rausch2019a} points out that edge resource management from the cloud is complex, partly because nodes may be on private networks or behind firewalls.
In addition, edge resources may have unreliable or limited network access, making it challenging to maintain constant communication channels.
The authors report that no out-of-the-box solution currently solves these issues, and request research into transparent edge-cloud resource consolidation/orchestration without the use of point-to-point integrations like VPNs.

\cite{Zhang2020} further proposes research into combining model-switching with performance optimization techniques like query caching and batching.
Several batching techniques have been reported for ML serving systems.
\cite{Crankshaw2017} uses a batching technique with a static maximum batch size with a timeout window which are both tuned on a per-model basis, while \cite{Choi2021} proposes an adaptive batching scheme specifically for neural networks, reducing latency during low-traffic scenarios.
Combining caching and batching with model-switching is an open problem for further work.

\cite{Zhang2020} proposes research into extending model-switching to additional types of computing resources.
CPUs are currently the most widely used for inference in existing MLaaS platforms, other types of hardware exist that are suited for neural network inference, such as GPUs, TPUs, etc.
Research into model-switching which takes into account these additional types of hardware resources should be conducted to identify possible performance benefits.
Further, the additional hardware heterogeneity could pose an interesting challenge when combining model-switching and hardware selection during dynamic query placement.

\cite{Rausch2019a} observes that the performance of the Kubernetes scheduler begins degrading with 5000 nodes and fewer than 10 constraints, struggling to process more than 15 functions per second.
It is conceivable that the number of edge devices may reach the thousands and beyond in an IoT scenario, e.g. in industry 4.0, suggesting that the scheduler may become a performance bottleneck in the future.
The authors therefore propose research into more scalable function scheduling.

\cite{Bosch2021} and \cite{Rausch2019} report a lack of generic solutions for deploying ML models to embedded and edge devices.
\cite{Rausch2019} reports that conventional IoT provisioning frameworks are not suited for deploying ML models, while \cite{Gupta2020} reports that there is a lack of solutions for deploying to edge devices that do not support containerization techniques.
\cite{Rausch2019a} reports that support for non-x86 architectures is lacking.

In the context of having multiple models deployed in an edge architecture, edge devices may have to evict models in order to free up resources.
\cite{Rausch2019a} proposes research into strategies for intelligently evicting models from edge devices.

According to \cite{Rausch2019}, monitoring edge-deployed ML models can potentially present two different challenges.
First, some metrics, such as concept drift and model drift, require continuous access to the original training set.
However, edge devices may not have access to the training set for multiple reasons, for example because of storage space constraints or because of data privacy concerns.
This requires further research into possible solutions.

Secondly, \cite{Rausch2019} identifies the problem of triggering centralized model retraining.
Retraining triggers require that monitoring data be continuously streamed from the edge devices to the (possibly centralized) ML pipeline.
Not only could this be difficult because of unreliable network connections, bandwidth restrictions or data usage constraints; handling large volumes of incoming monitoring data from edge devices may itself present a challenge of scalability.

\cite{Rausch2019} scalable provisioning