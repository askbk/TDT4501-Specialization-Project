\chapter{Research Results}
\label{ch:research_results}
This chapter presents the results from the SLR on a per-research question basis.

\section{RQ1: How is ML Model Deployment Handled in the State of the Art?}
\subsection{Packaging and Integration}
Packaging models in containers accessible through REST or RPC interfaces is a commonly reported method of integration, in which the model is wrapped in an API layer \cite{Garcia2020, Li2017, Ruf2021, Crankshaw2017}.
When deploying many models, using a consistent standard for APIs, such as OpenAPI\footnote{\url{https://www.openapis.org/}}, across models can facilitate system integration \cite{Garcia2020}.
Another reported method of integration is to serialize the model (e.g. into ONNX\footnote{\url{https://onnx.ai/}}, HDF5, joblib) and load it at runtime, possibly with an ML framework that is optimized for production (such as caffe2) \cite{Hazelwood2018, Peticolas2019, Paeaekkoenen2020, Chahal2020}.
The model could also be packaged in a format specific to an end-to-end framework such as MLflow \cite{Chen2020}.
The model may be integrated directly into the application code \cite{Liu2020, Ruf2021, Granlund2021}, traditionally first having to be rewritten for production \cite{Hazelwood2018}.

\subsection{Deployment}
Deployment, the transitioning of a  packaged and integrated model into a serving state, may occur through a few different methods.
Models packaged in containers are simply run directly as standalone services \cite{Liu2020,Ruf2021, Granlund2021, Li2017, Garcia2020}.
However, models may be deployed in a target environment that is different from where they are packaged, in which case the model transfer may happen through either a push- or a pull-pattern.

In a pull-pattern deployment, the target environment (host application running on e.g. server or edge device) periodically polls for model updates and downloads when available \cite{Paeaekkoenen2020, Li2017, Peticolas2019}.

In a push-pattern deployment, the target environment is notified of the availability of a new model by a master server, e.g. by the server where the model was trained.
This will happen either through a messaging service \cite{Liu2020, Garcia2020}, where message contains metadata including the location of the updated model, or by the model being pushed directly to the target environment through some receiving interface \cite{Paeaekkoenen2020}.

When an updated model has reached the target environment, subsequent redeployments can happen in a few different ways.
For models packaged as standalone containers, a container orchestration service (e.g. Kubernetes) can roll out updated models without downtime.
Serialized models may simply be loaded into memory by the application, potentially leading to downtime.
If the hosting application is containerized, model redeployment may be orchestrated by deploying a new application instance with the new model data, alongside the old instance.
When the new instance is finished initializing, it can start serving requests, and the old instance may be evicted \cite{Paeaekkoenen2020}.
To avoid high response latencies for the first query (cold-start issues), models may be queried with an empty request (or an explicit warm-up function call may be used if available) which forces lazy-loaded model components to initialize \cite{Li2017}.

When using an ML deployment service (e.g. SageMaker), the service may handle the deployment of serialized models \cite{Chahal2020}.

Models intended for batch predictions may simply be plugged into a computing pipeline such as Apache Spark\footnote{\url{https://spark.apache.org/}}, computing predictions and storing them in a database or data warehouse \cite{Li2017}.

\subsection{Serving and Inference}
Models are commonly made available for serving predictions through a REST \cite{Krishnamurthi2019, Liu2020, Ruf2021, Garcia2020, Crankshaw2017, Paeaekkoenen2020} or RPC \cite{Ruf2021, Li2017, Crankshaw2017} API.
To meet inference SLOs, there are several techniques reported.
One is model-switching, where less accurate but more performant models are used during periods of high load in order to meet required SLOs \cite{Zhang2020}.
Another is to use adaptive batching queues with a timeout, where queries are batched together in batch sizes that are tuned to the individual model and framework. During periods of low traffic, the timeout is reached before the query batch is filled, and inference is run on the batch in order to not exceed the SLO \cite{Crankshaw2017}.
Additionally, a cache layer may be put on top of the model to reduce computation \cite{Crankshaw2017}.
In order to avoid cold-start issues caused by model loading and initialization, models should be warmed up at deployment and be kept perpetually warm over the course of its life \cite{Zhang2020}.
Model warmup can be achieved through either a method provided by the ML framework \cite{Li2017}, or more generally through issuing an empty query against the model \cite{Garcia2020}.

\subsection{Monitoring and Logging}
Runtime monitoring is important for operationalized ML in order to increase trust and detect performance degradations \cite{Rausch2019, Li2017}.
When the model is operational, the whole application stack is constantly monitored for performance metrics such as latency, throughput, disk utilization, et cetera \cite{Ruf2021, Peticolas2019}.
Predictions are logged, and when available joined with actual outcomes\cite{Li2017}.
Model accuracy should be used to determine when a new model is needed \cite{Peticolas2019}.
The application should be validated against predefined KPIs of the project \cite{Ruf2021}.

\section{RQ2: What Are the Main Challenges in ML Model Deployment?}
\subsection{Packaging and Integration}
ML frameworks are not all created equal, and may be better suited for research than production or vice versa, typically because of performance characteristics versus support for rapid experimentation and debugging.
\cite{Crankshaw2017} notes several challenges in deploying ML frameworks: interface inconsistency across frameworks, changes over time in what is the best framework for the job, and frameworks that are not optimized for deployment.
Developers then have to make the choice of either using a suboptimal framework, or incurring technical debt in order to support and integrate multiple frameworks.

\cite{Hazelwood2018} reports solving this tradeoff by decoupling the frameworks from the model by transferring it from research to production using an exchange format for neural networks.
\cite{Crankshaw2017} also uses decoupling as a solution to this problem, but opts for adding an abstraction layer with a simple interface on top of all models.
This solution is more general, as it is not specific to neural networks.

\cite{Bosch2021} reports the extensive use of glue code as a challenge with which companies struggle.
Glue code is code that merely glues together different parts of the program, without itself providing any functionality.
The authors also highlight the integration of data-driven models with computation-driven software components as a potentially non-trivial task.

\subsection{Serving}
Serving-related challenges are some of the most often reported ones in the literature \cite{Bosch2021, Richins2020, Li2017, Baylor2017, Garcia2020, Crankshaw2017, Lwakatare2019, Bernardi2019, Yadwadkar2019, Chahal2020, Choi2021, Zhang2020, Gupta2020}, usually from a performance perspective.

Achieving a low inference latency and high throughput is widely reported as a challenge in serving ML models \cite{Bosch2021, Li2017,Crankshaw2017,Lwakatare2019,Bernardi2019}.
\textcite{Bernardi2019} found that there is a statistically significant negative correlation between latency and conversion rate at Booking.com, highlighting the business impact of serving performance.

Several potential solutions for improving serving performance have been proposed in the literature.
\textcite{Gupta2020} suggests deploying ML models to the edge in order to reduce network latency. However, edge deployment bring a whole host of its own challenges, which will be discussed in \cref{ch:results:rq2:edge}.

In a system utilizing publish/subscribe messaging services, \cite{Richins2020} observes that 33\% of application latency is a result of internal communication frameworks, and by experimentation find that the messaging system becomes congested when faced with an eightfold increase in inference performance.
With a growing number of specialized ML accelerator hardware, it not inconceivable for such performance bottlenecks to appear in the near future.
The authors find that a solution to the problem could be to increase the number of broker node instances used in the messaging system, which clears up the congestion issue. However, ML models typically do not make inferences based on raw data, but may require data transformations both before and after inference.
\cite{Richins2020} warns that the data processing code in streaming ML systems could soon become another performance bottleneck.
The authors do not, however, offer a solution.

Start-up/cold-start latency is a typical serving challenge for ML models \cite{Yadwadkar2019}. It is mainly caused by the ML model being loaded into memory and initialized, and is typically solved by warming up the model after it has been deployed, and then keeping it perpetually warm \cite{Zhang2020}.
There are, however, problems with this solution, which will be discussed in \cref{ch:research_results:rq4_gaps}.

\cite{Zhang2020} discusses the problem of meeting performance SLOs during significant variations in model query traffic, a problem also observed by \cite{Yadwadkar2019}.
The authors note that currently, the options are to either accept degraded throughput or latency during high-demand scenarios, or to scale up the hardware resources -- as suggested by \cite{Bernardi2019} -- and thereby incurring increased costs.
The difficulty in achieving cost-effective and performant inference is also corroborated by \cite{Chahal2020}, where the authors observe that increasing hardware instances and network bandwidth both improve inference latencies.
To solve the issue of upholding latency SLOs while minimizing costs, the model-switching technique is proposed by \cite{Zhang2020} as a solution, whereby high-accuracy/low-performance models are traded for lower-accuracy/high-performance models during load spikes in order to meet effective accuracy SLO, i.e. the average accuracy of predictions that meet latency requirements.



\subsection{Edge}
\label{ch:results:rq2:edge}

\subsection{Monitoring and Logging}
Monitoring ML models is challenging \cite{Li2017, Bernardi2019}, but is required in order to detect performance degradations and build confidence in the model \cite{Bosch2021}.
After making an inference, the true label may not be available for an extended period of time, making it difficult to monitor prediction quality \cite{Bernardi2019}.
One solution used in practice is to look at the distribution of predictions and determine if there is a significant deviation from what the response distribution of an ideal model would look like \cite{Bernardi2019}.

\section{RQ3: What Tools and Infrastructure Are Used to Deploy ML Models?}
Containerization is a commonly reported method of packaging ML models \cite{Ruf2021, Peticolas2019, Rausch2019}, and Docker is by far the most frequently reported solution \cite{Krishnamurthi2019, Richins2020, Li2017,Garcia2020,Crankshaw2017, Paeaekkoenen2020,Gupta2020}.

Models packaged in containers are typically accessed through either a REST API \cite{Garcia2020, Paeaekkoenen2020}, such as Flask \cite{Gupta2020}, or RPC interface \cite{Ruf2021, Crankshaw2017}, such as Apache Thrift \cite{Li2017}.

Having multiple containers deployed will often necessitate the use of a container orchestration tool such as Kubernetes \cite{Liu2020, Richins2020, Paeaekkoenen2020, Rausch2019a}.
However, when deploying on edge clusters, K3s may be preferrable \cite{Paeaekkoenen2020}.
Service for Kubernetes and K3s may be defined with service descriptors like Helm \cite{Paeaekkoenen2020}.

In order to manage multiple clusters of containers, a kubernetes-as-a-service tool such as Rancher may be used \cite{Paeaekkoenen2020}.

Apache OpenWhisk \cite{Garcia2020, Rausch2019a}.
KNative\cite{Rausch2019a}.

AWS S3 for model storage \cite{Liu2020, Chahal2020}.
Apache Kafka for messaging and communications \cite{Liu2020, Richins2020, Li2017,Garcia2020}.
TFX \cite{Ruf2021,Baylor2017}.
AWS deployment \cite{Krishnamurthi2019, Liu2020,Chahal2020}.
Apache Samza \cite{Li2017} for streaming engine.
Apache mesos execution framework for inference \cite{Garcia2020}.


AWS Greengrass \cite{Krishnamurthi2019}.
AWS SageMaker\cite{Ruf2021, Chahal2020}.
MLflow \cite{Chen2020, Ruf2021}.
Polyaxon \cite{Ruf2021}
Kedro\cite{Ruf2021}
ZenML\cite{Ruf2021}
H2O\cite{Ruf2021}
Kubeflow\cite{Ruf2021}
Flyte\cite{Ruf2021}
Seldon Core\cite{Ruf2021}
BentoML\cite{Ruf2021}

Airflow \cite{Ruf2021}
Prometheus \cite{Ruf2021}
Great Expectations \cite{Ruf2021}

\section{RQ4: What Are the Gaps in Current Tooling and Infrastructure Used to Deploy ML Models?}
\label{ch:research_results:rq4_gaps}

Many opportunities for further work and research have been identified in the reviewed literature.
In particular, \cite{Yadwadkar2019} and \cite{Zhang2020} identify a large number of research directions in the area of model serving performance.
\cite{Bosch2021}, \cite{Rausch2019} and \cite{Gupta2020} also propose several areas of research for edge ML deployment.

In the context of an ML system with multiple available variants of the same model (with different accuracy and performance characteristics) and multiple hardware resources to choose from, \cite{Yadwadkar2019} presents a series of interrelated open problems concerned with serving performance.
The automatic selection of model variant based on a given SLO will be referred to as \textit{model-switching} based on the terminology used in \cite{Zhang2020}. 
The overarching question is that of how the serving system should automatically select a model variant and underlying hardware given some SLO specified by a client application.

First, given an inference query, how should it be placed/scheduled by the serving system?
The options are to either generate a new model variant, to load an existing model variant, or to query an already loaded model.
The choice will depend on the individual application requirements, e.g. whether the inference is online or offline, what the SLOs are, etc.
\cite{Yadwadkar2019} and \cite{Zhang2020} propose that further research be conducted into how to synergistically combine model-switching and hardware resource selection.

Further, given the choice of placement, a new model may need to be loaded, additional hardware resources may need to be launched, or loaded models may need to be evicted to free up resources.
As \cite{Zhang2020} points out, keeping all models warm at all times is prohibitively expensive and scales poorly.
The model management operations add latency to the query response, affecting the system's ability to meet SLOs.
\cite{Yadwadkar2019} and \cite{Zhang2020} encourage research into methods for reducing or avoiding this latency, striving for the performance of perpetually warm models, while aiming for the resource usage of cold models.

Next, the question is how to capture and organize the data needed for making a decision on query placement.
In addition to the set of possible hardware resources and model variants, the serving system must also take into account the current state of the system, which is continually changing.
The current state of the system is described by which model variants are loaded on which hardware resources, which model variants exist but are not loaded, which model variants do not exist but are able to be generated, CPU/memory/disk/network usage of each hardware instance, inference query traffic.
All of this data needs to be captured and organized in a way that enables fast and efficient decision making on query placement.

Not only does the logical aspect of hardware and model variant selection need to be considered, the geographical location of hardware resources must also be taken into account.
Given that resources may be anywhere on the continuum of core-middle-edge, \cite{Yadwadkar2019} requests research into answering the questions of where models should be loaded, where resource management decisions should be made, and where query placement decisions should be made.
\cite{Rausch2019a} points out that edge resource management from the cloud is complex, partly because nodes may be on private networks or behind firewalls.
In addition, edge resources may have unreliable or limited network access, making it challenging to maintain constant communication channels.
The authors report that no out-of-the-box solution currently solves these issues, and request research into transparent edge-cloud resource consolidation/orchestration without the use of point-to-point integrations like VPNs.

\cite{Zhang2020} further proposes research into combining model-switching with performance optimization techniques like query caching and batching.
Several batching techniques have been reported for ML serving systems.
\cite{Crankshaw2017} uses a batching technique with a static maximum batch size with a timeout window which are both tuned on a per-model basis, while \cite{Choi2021} proposes an adaptive batching scheme specifically for neural networks, reducing latency during low-traffic scenarios.
Combining caching and batching with model-switching is an open problem for further work.

\cite{Zhang2020} proposes research into extending model-switching to additional types of computing resources.
CPUs are currently the most widely used for inference in existing MLaaS platforms, other types of hardware exist that are suited for neural network inference, such as GPUs, TPUs, etc.
Research into model-switching which takes into account these additional types of hardware resources should be conducted to identify possible performance benefits.
Further, the additional hardware heterogeneity could pose an interesting challenge when combining model-switching and hardware selection during dynamic query placement.

\cite{Rausch2019a} observes that the performance of the Kubernetes scheduler begins degrading with 5000 nodes and fewer than 10 constraints, struggling to process more than 15 functions per second.
It is conceivable that the number of edge devices may reach the thousands and beyond in an IoT scenario, e.g. in industry 4.0, suggesting that the scheduler may become a performance bottleneck in the future.
The authors therefore propose research into more scalable function scheduling.

\cite{Bosch2021} and \cite{Rausch2019} report a lack of generic solutions for deploying ML models to embedded and edge devices.
\cite{Rausch2019} reports that conventional IoT provisioning frameworks are not suited for deploying ML models, while \cite{Gupta2020} reports that there is a lack of solutions for deploying to edge devices that do not support containerization techniques.
\cite{Rausch2019a} reports that support for non-x86 architectures is lacking.

Edge devices present greater heterogeneity in computing capabilities, storage and networking, and in the context of having multiple models deployed in an edge architecture, some models may have to be evicted from time to time in order to free up resources for new deployments.
\cite{Rausch2019} and \cite{Rausch2019a} propose research into strategies for intelligently evicting models from edge devices.

According to \cite{Rausch2019}, monitoring edge-deployed ML models can potentially present two different challenges.
First, some metrics, such as concept drift and model drift, require continuous access to the original training set.
However, edge devices may not have access to the training set for multiple reasons, for example because of storage space constraints or because of data privacy concerns.
Further research is required to solve the problem of drift detection in edge devices where training data is unavailable.

Secondly, \cite{Rausch2019} identifies the problem of triggering centralized model retraining.
Retraining triggers require that monitoring data be continuously streamed from the edge devices to the (possibly centralized) ML pipeline.
Not only could this be difficult because of unreliable network connections, bandwidth restrictions or data usage constraints; handling large volumes of incoming monitoring data from edge devices may itself present a challenge of scalability.

On the topic of monitoring, \cite{Chen2020} highlights a need for framework-agnostic model telemetry solutions to enable the capture of statistical performance metrics with a broad set of supported deployment environments.

\cite{Chen2020} also notes the need for a general representation format for multistep ML workflows.
Many processes in ML pipelines contain multiple subtasks, where changing a single step may cause regressions.
A new format should provide explicit IO interfaces and enable parallell execution of independent subtasks with existing workflow execution systems (e.g. Airflow).

\section{Evaluation of Results}